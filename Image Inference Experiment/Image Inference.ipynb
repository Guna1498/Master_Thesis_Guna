{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6596ea08-4de9-4d3f-aea9-323689ba0ec3",
   "metadata": {},
   "source": [
    "# Model Name: Qwen/Qwen2.5-VL-7B-Instruct\n",
    "# Dataset: New Sloped Roofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7caa883f-3ba3-4288-a6fe-fc64786fe39a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 22 23:23:27 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:01:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             64W /  500W |       0MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |\n",
      "| N/A   58C    P0            371W /  500W |    6576MiB /  81920MiB |     99%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             64W /  500W |       0MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             67W /  500W |       0MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    1   N/A  N/A   1923877      C   ...iranda/.conda/envs/p310/bin/python3       6564MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51177ad9-d233-4b2b-bff2-c2e8908ad8c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Tutorial and details\n",
    "https://nodeshift.com/blog/how-to-install-qwen2-5-vl-7b-instruct-locally\n",
    "#### GPU Requirements\n",
    "1. Minimum: 24GB VRAM (with quantization)\r",
    "2. \n",
    "Recommended: 48GB+ VRAM for full performance (e.g., NVIDIA A6000 or A100\n",
    "3. \r\n",
    "Optimal: 80GB VRAM for long-context, video-heavy tasks (e.g., NVIDIA H10\n",
    "\n",
    "#### CPU Requirements\n",
    "1. Text-based tasks: 16-core CPU is sufficient.\r",
    "2. \n",
    "Multimodal (image/video) tasks: 32+ cores recommended for fast preprocessing\n",
    "\n",
    "#### RAM Requirements\n",
    "1. Minimum: 32GB RAM for smooth operation with images.\r",
    "2. \n",
    "Recommended: 64GB RAM for vision-heavy tasks.3. \r\n",
    "Optimal: 128GB+ RAM for handling long video\n",
    "\n",
    "#### Disk Space & Storage\n",
    "1. Minimum: 50GB free space for model weights and temporary files.\r",
    "2. \n",
    "Recommended: 1TB NVMe SSD for faster model loading and caching.3. \r\n",
    "High-speed SSD storage is crucial for video-heavy taskss..0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d002b5-bec2-4505-96e8-4aa778f9f37f",
   "metadata": {},
   "source": [
    "### Install PyTorch with GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "411adb2f-adff-4c00-87f0-ea0d432d9eb4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /mnt/home/gjenni/.local/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /mnt/home/gjenni/.local/lib/python3.12/site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in /mnt/home/gjenni/.local/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bfc66c-6b6a-4ed8-af83-c2ea5be46f0c",
   "metadata": {},
   "source": [
    "### Verify PyTorch Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb35df5-de67-40c8-aa99-ec7db5990a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "CUDA Available: True\n",
      "GPU Name: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e109432-0e34-408e-bcba-999d9db1896f",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49bd6687-3566-448b-8438-e168214d962d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/gjenni-tmpdir-Hbjvnq/pip-req-build-wzlw_1iv\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/gjenni-tmpdir-Hbjvnq/pip-req-build-wzlw_1iv\n",
      "  Resolved https://github.com/huggingface/transformers to commit c9d1e5238a752813ba91a8751a638a09b5efbb73\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /mnt/home/gjenni/.local/lib/python3.12/site-packages (1.4.0.dev0)\n",
      "Requirement already satisfied: filelock in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.51.0.dev0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.51.0.dev0) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.51.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from transformers==4.51.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from transformers==4.51.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.51.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from transformers==4.51.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.51.0.dev0) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.51.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.51.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.51.0.dev0) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.51.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->transformers==4.51.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->transformers==4.51.0.dev0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->transformers==4.51.0.dev0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->transformers==4.51.0.dev0) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.51.0.dev0-py3-none-any.whl size=11004959 sha256=ca6f3380eaa76cc868452b673464631c09ea4fc3781c5cfb8ede7ccc960eaa07\n",
      "  Stored in directory: /tmp/gjenni-tmpdir-Hbjvnq/pip-ephem-wheel-cache-7vhtt8ih/wheels/49/a7/50/c9fdabbf10e51bb1256adb0c1a587fedd7184f5bad28d47fe3\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.50.0.dev0\n",
      "    Uninstalling transformers-4.50.0.dev0:\n",
      "      Successfully uninstalled transformers-4.50.0.dev0\n",
      "Successfully installed transformers-4.51.0.dev0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting qwen-vl-utils==0.0.8 (from qwen-vl-utils[decord]==0.0.8)\n",
      "  Downloading qwen_vl_utils-0.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: av in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (14.1.0)\n",
      "Requirement already satisfied: packaging in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (24.1)\n",
      "Requirement already satisfied: pillow in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (10.4.0)\n",
      "Requirement already satisfied: requests in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2.32.3)\n",
      "Requirement already satisfied: decord in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from qwen-vl-utils[decord]==0.0.8) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from decord->qwen-vl-utils[decord]==0.0.8) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2024.7.4)\n",
      "Downloading qwen_vl_utils-0.0.8-py3-none-any.whl (5.9 kB)\n",
      "Installing collected packages: qwen-vl-utils\n",
      "  Attempting uninstall: qwen-vl-utils\n",
      "    Found existing installation: qwen-vl-utils 0.0.10\n",
      "    Uninstalling qwen-vl-utils-0.0.10:\n",
      "      Successfully uninstalled qwen-vl-utils-0.0.10\n",
      "Successfully installed qwen-vl-utils-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers accelerate\n",
    "!pip install qwen-vl-utils[decord]==0.0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3de24b-1eae-4c45-b08a-76cfa553cf0d",
   "metadata": {},
   "source": [
    "### Load the Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9634ae-2207-4b4a-a379-a4fdd063bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bffd80d6-91c8-4247-97eb-b615565c413b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d612ddb30224f28bdaa39c6e794a6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Processor Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "# Load the model on the GPU\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "print(\"Model and Processor Loaded Successfully!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34ca6e86-d388-414b-b56a-73be89739295",
   "metadata": {},
   "source": [
    "## Evaluation cases\n",
    "Both cases will be evaluated based on roof features and types for the various prompts.\n",
    "\n",
    "1. Case 1 (System Role: Helpful Assistant)\n",
    "2. Case 2 (System Role: Expert)\n",
    "   \n",
    "Based on the original ground truth values\n",
    "\n",
    "P1: Skylight\n",
    "\n",
    "P2: Vents/ Chimneys\n",
    "\n",
    "P3: Hipped Roof\n",
    "\n",
    "P4: Mixed (Hipped L)\n",
    "\n",
    "P5: Gable Roof\n",
    "\n",
    "P6: Mixed (Gable + Hipped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e055a-2d41-4597-a4ea-0fb4caf9777b",
   "metadata": {},
   "source": [
    "## Case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa15e7-0ab5-4300-b7f0-de6eb8e95ea9",
   "metadata": {},
   "source": [
    "### Image Inference for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3689edba-1b48-4171-b47a-77510a4bcc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output: ['system\\nYou are a helpful assistant.\\nuser\\nIs the roof of the building of hipped type?\\nassistant\\nNo, the roof of the building in the image is not of a hipped type. A hipped roof has four sloped sides that meet at the top, forming a hip. The roof in the image appears to be a gabled roof, which has two sloped sides that meet at the top and form a peak.']\n"
     ]
    }
   ],
   "source": [
    "# Load your image\n",
    "image_path = \"/mnt/data/oe215/env/guna/Sloped_249/3587751.png\"\n",
    "\n",
    "\n",
    "# Prepare input\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\", \"text\": \"Is the roof of the building of hipped type?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "# Convert input into the required format\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "# Tokenize and move to GPU\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "# Decode and print result\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(\"Generated Output:\", output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdafa1cb-011d-46a7-a1e1-b45ab23cf729",
   "metadata": {},
   "source": [
    "### Load GTs and Images and Inference Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e1ffb3-d7b1-401e-b275-18400816f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Excel and image folder\n",
    "excel_path = \"/mnt/data/oe215/env/guna/sloped_roof_249.xlsx\"\n",
    "image_folder = \"/mnt/data/oe215/env/guna/Sloped_249\"\n",
    "df = pd.read_excel(excel_path)\n",
    "df['FS_DE_ID'] = df['FS_DE_ID'].astype(str)\n",
    "\n",
    "def run_qwen_inference(image_path, prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Vision input\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate only the new tokens\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Decode only the newly generated part\n",
    "    generated_text = processor.batch_decode(\n",
    "        generated_ids[:, inputs['input_ids'].shape[1]:],  # â¬…ï¸ Skip the prompt tokens\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )[0]\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac34ad-1ae6-4b13-b805-a5d0dfce1582",
   "metadata": {},
   "source": [
    "### Run Inference for Skylights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0ca97c8-d41b-4ad5-9aa7-7ee27bc6425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/249 [00:00<?, ?it/s]/mnt/home/gjenni/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:41<00:00,  6.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prompt = \"Does the roof of the building have any skylights? Answer with only 'Yes' or 'No' by properly analyzing the image?\"\n",
    "skylight_results = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    fs_id = row[\"FS_DE_ID\"]\n",
    "    image_path = os.path.join(image_folder, f\"{fs_id}.png\")\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            response = run_qwen_inference(image_path, prompt)\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "    else:\n",
    "        response = \"Image not found\"\n",
    "\n",
    "    skylight_results.append({\n",
    "        \"FS_DE_ID\": fs_id,\n",
    "        \"Ground Truth\": row[\"Skylights\"],\n",
    "        \"Prediction\": response\n",
    "    })\n",
    "\n",
    "skylight_df = pd.DataFrame(skylight_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f858151b-8c06-4cc5-a084-dcbdc2b224c9",
   "metadata": {},
   "source": [
    "### Evaluate Predictions + Export CSV and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "046cd705-06e7-4f07-a658-874cbfa0f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Directories to save outputs\n",
    "csv_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions\"\n",
    "cm_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix\"\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "def evaluate_and_save_binary(df, name, gt_col=\"Ground Truth\", pred_col=\"Prediction\"):\n",
    "    df[\"Prediction_Normalized\"] = df[pred_col].str.strip().str.lower().str.extract(r'(yes|no)')[0].map({\"yes\": \"Y\", \"no\": \"N\"})\n",
    "    eval_df = df.dropna(subset=[\"Prediction_Normalized\"]).copy()\n",
    "    eval_df[\"Ground Truth Binary\"] = eval_df[gt_col].map({\"Y\": 1, \"N\": 0})\n",
    "    eval_df[\"Prediction Binary\"] = eval_df[\"Prediction_Normalized\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "    y_true = eval_df[\"Ground Truth Binary\"]\n",
    "    y_pred = eval_df[\"Prediction Binary\"]\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"No\", \"Yes\"], zero_division=0))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"], cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    cm_path = os.path.join(cm_output_dir, f\"{name.lower().replace(' ', '_')}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save CSV\n",
    "    export_df = eval_df[[\"FS_DE_ID\", \"Ground Truth Binary\", \"Prediction Binary\"]]\n",
    "    csv_path = os.path.join(csv_output_dir, f\"{name.lower().replace(' ', '_')}_predictions.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Saved to:\\n- {csv_path}\\n- {cm_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95815d8b-9683-47c2-9324-633dbf2b0c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SKYLIGHTS ===\n",
      "Accuracy: 0.6265060240963856\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.72      0.77      0.75       177\n",
      "         Yes       0.32      0.26      0.29        72\n",
      "\n",
      "    accuracy                           0.63       249\n",
      "   macro avg       0.52      0.52      0.52       249\n",
      "weighted avg       0.61      0.63      0.61       249\n",
      "\n",
      "âœ… Saved to:\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions/skylights_predictions.csv\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix/skylights_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_save_binary(skylight_df, \"Skylights\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b10d7d4-3665-42dc-9abc-46541dd63baa",
   "metadata": {},
   "source": [
    "### Dormer Windows inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02c5222-fdc2-4cf6-bccf-ae889f0a2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/249 [00:00<?, ?it/s]/mnt/home/gjenni/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:40<00:00,  6.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# ðŸ§  Updated prompt for Dormer Windows\n",
    "prompt = \"Does the roof of the building have any dormer windows? Answer with only 'Yes' or 'No' by properly analyzing the image.\"\n",
    "\n",
    "dormer_results = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    fs_id = row[\"FS_DE_ID\"]\n",
    "    image_path = os.path.join(image_folder, f\"{fs_id}.png\")\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            response = run_qwen_inference(image_path, prompt)\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "    else:\n",
    "        response = \"Image not found\"\n",
    "\n",
    "    dormer_results.append({\n",
    "        \"FS_DE_ID\": fs_id,\n",
    "        \"Ground Truth\": row[\"Dormer Windows\"],\n",
    "        \"Prediction\": response\n",
    "    })\n",
    "\n",
    "dormer_df = pd.DataFrame(dormer_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9206c4a-db0c-45cc-b8fa-9707d7467488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Directories to save outputs\n",
    "csv_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions\"\n",
    "cm_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix\"\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "def evaluate_and_save_binary(df, name, gt_col=\"Ground Truth\", pred_col=\"Prediction\"):\n",
    "    # Normalize predictions\n",
    "    df[\"Prediction_Normalized\"] = df[pred_col].str.strip().str.lower().str.extract(r'(yes|no)')[0].map({\"yes\": \"Y\", \"no\": \"N\"})\n",
    "\n",
    "    # Drop any rows with missing prediction or ground truth\n",
    "    before = len(df)\n",
    "    eval_df = df.dropna(subset=[\"Prediction_Normalized\", gt_col]).copy()\n",
    "    dropped = before - len(eval_df)\n",
    "\n",
    "    if dropped > 0:\n",
    "        print(f\"âš ï¸ Dropped {dropped} row(s) due to missing values in prediction or ground truth.\")\n",
    "\n",
    "    # Binary conversion\n",
    "    eval_df[\"Ground Truth Binary\"] = eval_df[gt_col].map({\"Y\": 1, \"N\": 0})\n",
    "    eval_df[\"Prediction Binary\"] = eval_df[\"Prediction_Normalized\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "    y_true = eval_df[\"Ground Truth Binary\"]\n",
    "    y_pred = eval_df[\"Prediction Binary\"]\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"No\", \"Yes\"], zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"], cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    cm_path = os.path.join(cm_output_dir, f\"{name.lower().replace(' ', '_')}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save to CSV\n",
    "    export_df = eval_df[[\"FS_DE_ID\", \"Ground Truth Binary\", \"Prediction Binary\"]]\n",
    "    csv_path = os.path.join(csv_output_dir, f\"{name.lower().replace(' ', '_')}_predictions.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Saved to:\\n- {csv_path}\\n- {cm_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be126cf2-ab71-4e8b-bdf3-13793f339778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DORMER WINDOWS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/gjenni/.local/lib/python3.12/site-packages/sklearn/utils/_array_api.py:390: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype, copy=copy, casting=casting)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y_true contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate_and_save_binary(dormer_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDormer Windows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m, in \u001b[0;36mevaluate_and_save_binary\u001b[0;34m(df, name, gt_col, pred_col)\u001b[0m\n\u001b[1;32m     28\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m eval_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction Binary\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_true, y_pred))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_true, y_pred, target_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m\"\u001b[39m], zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Confusion Matrix\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:231\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    229\u001b[0m xp, _, device \u001b[38;5;241m=\u001b[39m get_namespace_and_device(y_true, y_pred, sample_weight)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    232\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:104\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m    103\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m--> 104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/multiclass.py:398\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    396\u001b[0m     data \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39many(data \u001b[38;5;241m!=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(data, \u001b[38;5;28mint\u001b[39m)):\n\u001b[0;32m--> 398\u001b[0m         _assert_all_finite(data, input_name\u001b[38;5;241m=\u001b[39minput_name)\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    124\u001b[0m     X,\n\u001b[1;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    130\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y_true contains NaN."
     ]
    }
   ],
   "source": [
    "evaluate_and_save_binary(dormer_df, \"Dormer Windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ff5f9-1ae0-4397-9055-b9e7d21533f2",
   "metadata": {},
   "source": [
    "### Vents Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2b899d9-28d0-4822-961b-0d6491d1cc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/249 [00:00<?, ?it/s]/mnt/home/gjenni/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:40<00:00,  6.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "prompt = \"Does the roof of the building have any vents or chimneys? Answer with only 'Yes' or 'No' by properly analyzing the image?\"\n",
    "\n",
    "vents_results = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    fs_id = row[\"FS_DE_ID\"]\n",
    "    image_path = os.path.join(image_folder, f\"{fs_id}.png\")\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            response = run_qwen_inference(image_path, prompt)\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "    else:\n",
    "        response = \"Image not found\"\n",
    "\n",
    "    vents_results.append({\n",
    "        \"FS_DE_ID\": fs_id,\n",
    "        \"Ground Truth\": row[\"Vents/Chimneys\"],\n",
    "        \"Prediction\": response\n",
    "    })\n",
    "\n",
    "vents_df = pd.DataFrame(vents_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f24dfff-b43f-4b1c-8963-2052ce5df202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Directories to save outputs\n",
    "csv_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions\"\n",
    "cm_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix\"\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "def evaluate_and_save_binary(df, name, gt_col=\"Ground Truth\", pred_col=\"Prediction\"):\n",
    "    df[\"Prediction_Normalized\"] = df[pred_col].str.strip().str.lower().str.extract(r'(yes|no)')[0].map({\"yes\": \"Y\", \"no\": \"N\"})\n",
    "    eval_df = df.dropna(subset=[\"Prediction_Normalized\"]).copy()\n",
    "    eval_df[\"Ground Truth Binary\"] = eval_df[gt_col].map({\"Y\": 1, \"N\": 0})\n",
    "    eval_df[\"Prediction Binary\"] = eval_df[\"Prediction_Normalized\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "    y_true = eval_df[\"Ground Truth Binary\"]\n",
    "    y_pred = eval_df[\"Prediction Binary\"]\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"No\", \"Yes\"], zero_division=0))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"], cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    cm_path = os.path.join(cm_output_dir, f\"{name.lower().replace(' ', '_')}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save CSV\n",
    "    export_df = eval_df[[\"FS_DE_ID\", \"Ground Truth Binary\", \"Prediction Binary\"]]\n",
    "    csv_path = os.path.join(csv_output_dir, f\"{name.lower().replace(' ', '_')}_predictions.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Saved to:\\n- {csv_path}\\n- {cm_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2232bc13-2504-46ad-831a-f2f2d5b38c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VENTS ===\n",
      "Accuracy: 0.321285140562249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.09      0.94      0.16        17\n",
      "         Yes       0.98      0.28      0.43       232\n",
      "\n",
      "    accuracy                           0.32       249\n",
      "   macro avg       0.54      0.61      0.30       249\n",
      "weighted avg       0.92      0.32      0.41       249\n",
      "\n",
      "âœ… Saved to:\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions/vents_predictions.csv\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix/vents_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_save_binary(vents_df, \"Vents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14756bd6-5d21-443b-8673-ca1861ee8fcc",
   "metadata": {},
   "source": [
    "### Run Inference for Hipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b4f0207-9297-49af-9cf5-4dd3bf0ab115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:40<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prompt = \"Is the roof of the building of hipped type? Answer with only 'Yes' or 'No' by properly analyzing the image?\"\n",
    "\n",
    "hipped_results = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    fs_id = row[\"FS_DE_ID\"]\n",
    "    image_path = os.path.join(image_folder, f\"{fs_id}.png\")\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            response = run_qwen_inference(image_path, prompt)\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "    else:\n",
    "        response = \"Image not found\"\n",
    "\n",
    "    hipped_results.append({\n",
    "        \"FS_DE_ID\": fs_id,\n",
    "        \"Ground Truth\": row[\"Hipped\"],\n",
    "        \"Prediction\": response\n",
    "    })\n",
    "\n",
    "hipped_df = pd.DataFrame(hipped_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf7c76f-527f-4cdd-8462-9a5bbef2b5d5",
   "metadata": {},
   "source": [
    "### Hipped Evaluate Predictions + Export CSV and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7dac5e28-0d8b-426e-9491-1377a10a9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Save directories\n",
    "csv_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions\"\n",
    "cm_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix\"\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… Improved Yes/No extractor\n",
    "def extract_yes_no(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    text = text.strip().lower()\n",
    "    if 'yes' in text and 'no' not in text:\n",
    "        return 'Y'\n",
    "    elif 'no' in text and 'yes' not in text:\n",
    "        return 'N'\n",
    "    return None  # ambiguous or both present\n",
    "\n",
    "def evaluate_and_save_binary(df, name, gt_col=\"Ground Truth\", pred_col=\"Prediction\"):\n",
    "    df[\"Prediction_Normalized\"] = df[pred_col].apply(extract_yes_no)\n",
    "\n",
    "    # Filter rows where prediction could be extracted\n",
    "    eval_df = df.dropna(subset=[\"Prediction_Normalized\"]).copy()\n",
    "\n",
    "    if eval_df.empty:\n",
    "        print(f\"\\nðŸš« Skipping '{name}' â€” no valid yes/no predictions found.\")\n",
    "        return\n",
    "\n",
    "    # Convert Y/N to 1/0\n",
    "    eval_df[\"Ground Truth Binary\"] = eval_df[gt_col].map({\"Y\": 1, \"N\": 0})\n",
    "    eval_df[\"Prediction Binary\"] = eval_df[\"Prediction_Normalized\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "    y_true = eval_df[\"Ground Truth Binary\"]\n",
    "    y_pred = eval_df[\"Prediction Binary\"]\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"No\", \"Yes\"], zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"], cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    cm_path = os.path.join(cm_output_dir, f\"{name.lower().replace(' ', '_')}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save predictions\n",
    "    export_df = eval_df[[\"FS_DE_ID\", \"Ground Truth Binary\", \"Prediction Binary\"]]\n",
    "    csv_path = os.path.join(csv_output_dir, f\"{name.lower().replace(' ', '_')}_predictions.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Saved to:\\n- {csv_path}\\n- {cm_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "154fcc5b-6601-46e1-a524-a7d4e6e663fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HIPPED ===\n",
      "Accuracy: 0.678714859437751\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.79      0.70      0.74       164\n",
      "         Yes       0.52      0.65      0.58        85\n",
      "\n",
      "    accuracy                           0.68       249\n",
      "   macro avg       0.66      0.67      0.66       249\n",
      "weighted avg       0.70      0.68      0.69       249\n",
      "\n",
      "âœ… Saved to:\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions/hipped_predictions.csv\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix/hipped_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_save_binary(hipped_df, \"Hipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc805874-4c26-43d1-b0d2-1f89f6992e4f",
   "metadata": {},
   "source": [
    "### Run Inference for Mixed Hipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc96d03a-d359-47c1-83f5-66fea3673635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/249 [00:00<?, ?it/s]/mnt/home/gjenni/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:40<00:00,  6.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# ðŸ”„ Prompt tailored for mixed L-shaped hipped roof detection\n",
    "prompt = \"Is the roof of the building of Mixed (Hipped L) type? Answer with only 'Yes' or 'No' by properly analyzing the image?\"\n",
    "\n",
    "mixed_hipped_results = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    fs_id = row[\"FS_DE_ID\"]\n",
    "    image_path = os.path.join(image_folder, f\"{fs_id}.png\")\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            response = run_qwen_inference(image_path, prompt)\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "    else:\n",
    "        response = \"Image not found\"\n",
    "\n",
    "    mixed_hipped_results.append({\n",
    "        \"FS_DE_ID\": fs_id,\n",
    "        \"Ground Truth\": row[\"Mixed (Hipped L)\"],\n",
    "        \"Prediction\": response\n",
    "    })\n",
    "\n",
    "mixed_hipped_df = pd.DataFrame(mixed_hipped_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39671f3b-68bb-460b-bca2-d652f9fb6def",
   "metadata": {},
   "source": [
    "### Mixed (Hipped L) Evaluate Predictions + Export CSV and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f683de6f-aec4-4ed5-8ee3-d0550f0dc09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Save directories\n",
    "csv_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions\"\n",
    "cm_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix\"\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… Improved Yes/No extractor\n",
    "def extract_yes_no(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    text = text.strip().lower()\n",
    "    if 'yes' in text and 'no' not in text:\n",
    "        return 'Y'\n",
    "    elif 'no' in text and 'yes' not in text:\n",
    "        return 'N'\n",
    "    return None  # ambiguous or both present\n",
    "\n",
    "def evaluate_and_save_binary(df, name, gt_col=\"Ground Truth\", pred_col=\"Prediction\"):\n",
    "    df[\"Prediction_Normalized\"] = df[pred_col].apply(extract_yes_no)\n",
    "\n",
    "    # Filter rows where prediction could be extracted\n",
    "    eval_df = df.dropna(subset=[\"Prediction_Normalized\"]).copy()\n",
    "\n",
    "    if eval_df.empty:\n",
    "        print(f\"\\nðŸš« Skipping '{name}' â€” no valid yes/no predictions found.\")\n",
    "        return\n",
    "\n",
    "    # Convert Y/N to 1/0\n",
    "    eval_df[\"Ground Truth Binary\"] = eval_df[gt_col].map({\"Y\": 1, \"N\": 0})\n",
    "    eval_df[\"Prediction Binary\"] = eval_df[\"Prediction_Normalized\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "    y_true = eval_df[\"Ground Truth Binary\"]\n",
    "    y_pred = eval_df[\"Prediction Binary\"]\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"No\", \"Yes\"], zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"], cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    cm_path = os.path.join(cm_output_dir, f\"{name.lower().replace(' ', '_')}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save predictions\n",
    "    export_df = eval_df[[\"FS_DE_ID\", \"Ground Truth Binary\", \"Prediction Binary\"]]\n",
    "    csv_path = os.path.join(csv_output_dir, f\"{name.lower().replace(' ', '_')}_predictions.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Saved to:\\n- {csv_path}\\n- {cm_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43165a5e-707e-45a7-8f47-2f68676d3e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MIXED (HIPPED L) ===\n",
      "Accuracy: 0.6626506024096386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.88      0.67      0.76       199\n",
      "         Yes       0.33      0.64      0.43        50\n",
      "\n",
      "    accuracy                           0.66       249\n",
      "   macro avg       0.60      0.65      0.60       249\n",
      "weighted avg       0.77      0.66      0.69       249\n",
      "\n",
      "âœ… Saved to:\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions/mixed_(hipped_l)_predictions.csv\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix/mixed_(hipped_l)_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_save_binary(mixed_hipped_df, \"Mixed (Hipped L)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac13960-b5ae-49bd-8d35-3faead56451c",
   "metadata": {},
   "source": [
    "### Run Inference for Gable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c8de668-0307-43eb-9ab1-257e763ab74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/249 [00:00<?, ?it/s]/mnt/home/gjenni/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:40<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prompt = \"Does the roof of the building have two sloping sides that form a triangular wall on both ends (gable roof)? Answer only with 'yes' or 'no'.\"\n",
    "\n",
    "gable_results = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    fs_id = row[\"FS_DE_ID\"]\n",
    "    image_path = os.path.join(image_folder, f\"{fs_id}.png\")\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            response = run_qwen_inference(image_path, prompt)\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "    else:\n",
    "        response = \"Image not found\"\n",
    "\n",
    "    gable_results.append({\n",
    "        \"FS_DE_ID\": fs_id,\n",
    "        \"Ground Truth\": row[\"Gable\"],\n",
    "        \"Prediction\": response\n",
    "    })\n",
    "\n",
    "gable_df = pd.DataFrame(gable_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b629ef-19ec-4831-aa62-ce0da8a90ed1",
   "metadata": {},
   "source": [
    "### Gable Evaluate Predictions + Export CSV and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72c2c4bc-e6f7-4f1f-a074-70639b00a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Save directories\n",
    "csv_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions\"\n",
    "cm_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix\"\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… Improved Yes/No extractor\n",
    "def extract_yes_no(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    text = text.strip().lower()\n",
    "    if 'yes' in text and 'no' not in text:\n",
    "        return 'Y'\n",
    "    elif 'no' in text and 'yes' not in text:\n",
    "        return 'N'\n",
    "    return None  # ambiguous or both present\n",
    "\n",
    "def evaluate_and_save_binary(df, name, gt_col=\"Ground Truth\", pred_col=\"Prediction\"):\n",
    "    df[\"Prediction_Normalized\"] = df[pred_col].apply(extract_yes_no)\n",
    "\n",
    "    # Filter rows where prediction could be extracted\n",
    "    eval_df = df.dropna(subset=[\"Prediction_Normalized\"]).copy()\n",
    "\n",
    "    if eval_df.empty:\n",
    "        print(f\"\\nðŸš« Skipping '{name}' â€” no valid yes/no predictions found.\")\n",
    "        return\n",
    "\n",
    "    # Convert Y/N to 1/0\n",
    "    eval_df[\"Ground Truth Binary\"] = eval_df[gt_col].map({\"Y\": 1, \"N\": 0})\n",
    "    eval_df[\"Prediction Binary\"] = eval_df[\"Prediction_Normalized\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "    y_true = eval_df[\"Ground Truth Binary\"]\n",
    "    y_pred = eval_df[\"Prediction Binary\"]\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"No\", \"Yes\"], zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"], cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    cm_path = os.path.join(cm_output_dir, f\"{name.lower().replace(' ', '_')}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save predictions\n",
    "    export_df = eval_df[[\"FS_DE_ID\", \"Ground Truth Binary\", \"Prediction Binary\"]]\n",
    "    csv_path = os.path.join(csv_output_dir, f\"{name.lower().replace(' ', '_')}_predictions.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Saved to:\\n- {csv_path}\\n- {cm_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efdd9db7-8c73-42aa-86cc-dea88e788a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GABLE ===\n",
      "Accuracy: 0.3815261044176707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.36      0.03      0.05       151\n",
      "         Yes       0.38      0.93      0.54        98\n",
      "\n",
      "    accuracy                           0.38       249\n",
      "   macro avg       0.37      0.48      0.30       249\n",
      "weighted avg       0.37      0.38      0.24       249\n",
      "\n",
      "âœ… Saved to:\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions/gable_predictions.csv\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix/gable_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_save_binary(gable_df, \"Gable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e3dca0-44ee-4795-8ab1-e8dcc52b5325",
   "metadata": {},
   "source": [
    "### Run Inference for Mixed(Gable+Hipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49ae490f-8af7-474b-ac55-1d8fd4017a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/249 [00:00<?, ?it/s]/mnt/home/gjenni/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:40<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prompt = \"Does the roof of the building combine both gable and hipped sections, such as a mix of triangular gable ends and hips on other sides? Answer only 'yes' or 'no'.\"\n",
    "\n",
    "mixed_gable_results = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    fs_id = row[\"FS_DE_ID\"]\n",
    "    image_path = os.path.join(image_folder, f\"{fs_id}.png\")\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            response = run_qwen_inference(image_path, prompt)\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "    else:\n",
    "        response = \"Image not found\"\n",
    "\n",
    "    mixed_gable_results.append({\n",
    "        \"FS_DE_ID\": fs_id,\n",
    "        \"Ground Truth\": row[\"Mixed(Gable+Hipped)\"],\n",
    "        \"Prediction\": response\n",
    "    })\n",
    "\n",
    "mixed_gable_df = pd.DataFrame(mixed_gable_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2430a6f-72e4-451f-b505-fc92da768403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Save directories\n",
    "csv_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions\"\n",
    "cm_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix\"\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… Improved Yes/No extractor\n",
    "def extract_yes_no(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    text = text.strip().lower()\n",
    "    if 'yes' in text and 'no' not in text:\n",
    "        return 'Y'\n",
    "    elif 'no' in text and 'yes' not in text:\n",
    "        return 'N'\n",
    "    return None  # ambiguous or both present\n",
    "\n",
    "def evaluate_and_save_binary(df, name, gt_col=\"Ground Truth\", pred_col=\"Prediction\"):\n",
    "    df[\"Prediction_Normalized\"] = df[pred_col].apply(extract_yes_no)\n",
    "\n",
    "    # Filter rows where prediction could be extracted\n",
    "    eval_df = df.dropna(subset=[\"Prediction_Normalized\"]).copy()\n",
    "\n",
    "    if eval_df.empty:\n",
    "        print(f\"\\nðŸš« Skipping '{name}' â€” no valid yes/no predictions found.\")\n",
    "        return\n",
    "\n",
    "    # Convert Y/N to 1/0\n",
    "    eval_df[\"Ground Truth Binary\"] = eval_df[gt_col].map({\"Y\": 1, \"N\": 0})\n",
    "    eval_df[\"Prediction Binary\"] = eval_df[\"Prediction_Normalized\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "    y_true = eval_df[\"Ground Truth Binary\"]\n",
    "    y_pred = eval_df[\"Prediction Binary\"]\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"No\", \"Yes\"], zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"], cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    cm_path = os.path.join(cm_output_dir, f\"{name.lower().replace(' ', '_')}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save predictions\n",
    "    export_df = eval_df[[\"FS_DE_ID\", \"Ground Truth Binary\", \"Prediction Binary\"]]\n",
    "    csv_path = os.path.join(csv_output_dir, f\"{name.lower().replace(' ', '_')}_predictions.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Saved to:\\n- {csv_path}\\n- {cm_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "049bbc36-b260-4d1d-b51d-9f53e9d284ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MIXED(GABLE+HIPPED) ===\n",
      "Accuracy: 0.10040160642570281\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       1.00      0.04      0.07       233\n",
      "         Yes       0.07      1.00      0.12        16\n",
      "\n",
      "    accuracy                           0.10       249\n",
      "   macro avg       0.53      0.52      0.10       249\n",
      "weighted avg       0.94      0.10      0.08       249\n",
      "\n",
      "âœ… Saved to:\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions/mixed(gable+hipped)_predictions.csv\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix/mixed(gable+hipped)_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_save_binary(mixed_gable_df, \"Mixed(Gable+Hipped)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d0e43-bafe-48ec-9b29-b873c8442bce",
   "metadata": {},
   "source": [
    "## Living Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9245a35c-c7b0-4d7d-b714-2634fd519c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/249 [00:00<?, ?it/s]/mnt/home/gjenni/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [05:26<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prompt = \"Analyze the image and determine if there is living space under the roof of the building. Consider roof features like gabled or hipped roof, dormer windows, skylights, and overall roof volume. Answer 'Yes' if the roof has dormer windows or skylights or gabled, otherwise answer 'No.'\"\n",
    "\n",
    "\n",
    "living_results = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    fs_id = row[\"FS_DE_ID\"]\n",
    "    image_path = os.path.join(image_folder, f\"{fs_id}.png\")\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            response = run_qwen_inference(image_path, prompt)\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "    else:\n",
    "        response = \"Image not found\"\n",
    "\n",
    "    living_results.append({\n",
    "        \"FS_DE_ID\": fs_id,\n",
    "        \"Ground Truth\": row[\"Presence of living space\"],\n",
    "        \"Prediction\": response\n",
    "    })\n",
    "\n",
    "living_df = pd.DataFrame(living_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a9a14c3-87e3-40e5-9468-b126a74ea9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Save directories\n",
    "csv_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions\"\n",
    "cm_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix\"\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… Improved Yes/No extractor\n",
    "def extract_yes_no(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    text = text.strip().lower()\n",
    "    if 'yes' in text and 'no' not in text:\n",
    "        return 'Y'\n",
    "    elif 'no' in text and 'yes' not in text:\n",
    "        return 'N'\n",
    "    return None  # ambiguous or both present\n",
    "\n",
    "def evaluate_and_save_binary(df, name, gt_col=\"Ground Truth\", pred_col=\"Prediction\"):\n",
    "    df[\"Prediction_Normalized\"] = df[pred_col].apply(extract_yes_no)\n",
    "\n",
    "    # Filter rows where prediction could be extracted\n",
    "    eval_df = df.dropna(subset=[\"Prediction_Normalized\"]).copy()\n",
    "\n",
    "    if eval_df.empty:\n",
    "        print(f\"\\nðŸš« Skipping '{name}' â€” no valid yes/no predictions found.\")\n",
    "        return\n",
    "\n",
    "    # Convert Y/N to 1/0\n",
    "    eval_df[\"Ground Truth Binary\"] = eval_df[gt_col].map({\"Y\": 1, \"N\": 0})\n",
    "    eval_df[\"Prediction Binary\"] = eval_df[\"Prediction_Normalized\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "    y_true = eval_df[\"Ground Truth Binary\"]\n",
    "    y_pred = eval_df[\"Prediction Binary\"]\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"No\", \"Yes\"], zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"], cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    cm_path = os.path.join(cm_output_dir, f\"{name.lower().replace(' ', '_')}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save predictions\n",
    "    export_df = eval_df[[\"FS_DE_ID\", \"Ground Truth Binary\", \"Prediction Binary\"]]\n",
    "    csv_path = os.path.join(csv_output_dir, f\"{name.lower().replace(' ', '_')}_predictions.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Saved to:\\n- {csv_path}\\n- {cm_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e32ade8-6e4b-4230-bb46-cf728ac10caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LIVING SPACE ===\n",
      "Accuracy: 0.3614457831325301\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.33      0.93      0.49        83\n",
      "         Yes       0.68      0.08      0.14       166\n",
      "\n",
      "    accuracy                           0.36       249\n",
      "   macro avg       0.51      0.50      0.32       249\n",
      "weighted avg       0.57      0.36      0.26       249\n",
      "\n",
      "âœ… Saved to:\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions/living_space_predictions.csv\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix/living_space_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_save_binary(living_df, \"Living Space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d2f496-6184-4df7-a9d5-a98d1ee02889",
   "metadata": {},
   "source": [
    "## New Combined Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce08dce-042b-49b1-9b7d-a7abfdac4e3d",
   "metadata": {},
   "source": [
    "### Combining Hipped and Mixed Hipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db2302b0-02b5-4848-9a9c-0fab7d4442c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Combined labels saved as 'combined_hipped_gable_labels.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your Excel file (already in JupyterLab directory)\n",
    "df = pd.read_excel(\"sloped_roof_249.xlsx\")\n",
    "\n",
    "# Combine 'Hipped' and 'Mixed (Hipped L)' into 'Combined_Hipped'\n",
    "df['Combined_Hipped'] = df.apply(\n",
    "    lambda row: 'Y' if row.get('Hipped') == 'Y' or row.get('Mixed (Hipped L)') == 'Y' else 'N',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine 'Gable' and 'Mixed(Gable+Hipped)' into 'Combined_Gable'\n",
    "df['Combined_Gable'] = df.apply(\n",
    "    lambda row: 'Y' if row.get('Gable') == 'Y' or row.get('Mixed(Gable+Hipped)') == 'Y' else 'N',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save to new CSV\n",
    "df[['FS_DE_ID', 'Combined_Hipped', 'Combined_Gable']].to_csv(\"combined_hipped_gable_labels.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Combined labels saved as 'combined_hipped_gable_labels.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7169386e-d8f3-4cbd-aa7d-9c7a04f6d551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pie chart for Combined_Hipped\n",
    "hipped_counts = df['Combined_Hipped'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Combined Hipped Pie Chart\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(\n",
    "    hipped_counts,\n",
    "    labels=hipped_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=['skyblue', 'navy'],\n",
    "    startangle=140\n",
    ")\n",
    "plt.title('Combined Hipped - Class Distribution (Y vs N)')\n",
    "\n",
    "# Pie chart for Combined_Gable\n",
    "gable_counts = df['Combined_Gable'].value_counts()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(\n",
    "    gable_counts,\n",
    "    labels=gable_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=['lightgreen', 'darkgreen'],\n",
    "    startangle=140\n",
    ")\n",
    "plt.title('Combined Gable - Class Distribution (Y vs N)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b41a9-1757-40a9-8eea-ba46554cf555",
   "metadata": {},
   "source": [
    "### Loading the combined labels of hipped roof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1167b47f-4b31-4b89-91dc-264a9df976ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the combined CSV file\n",
    "combined_path = \"combined_hipped_gable_labels.csv\"\n",
    "df_combined = pd.read_csv(combined_path)\n",
    "\n",
    "# Confirm FS_DE_ID is string type\n",
    "df_combined['FS_DE_ID'] = df_combined['FS_DE_ID'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdbcc64-614a-4c2d-a24a-15490ac2204c",
   "metadata": {},
   "source": [
    "### Inference for combined hipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3e67bc4-bb06-4611-985e-781389239c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/249 [00:00<?, ?it/s]/mnt/home/gjenni/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:40<00:00,  6.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Prompt (use your best-performing one)\n",
    "prompt = \"Is the roof of the building of hipped type? Answer with only 'Yes' or 'No' by properly analyzing the image?\"\n",
    "\n",
    "combined_hipped_results = []\n",
    "\n",
    "for idx, row in tqdm(df_combined.iterrows(), total=len(df_combined)):\n",
    "    fs_id = row[\"FS_DE_ID\"]\n",
    "    image_path = os.path.join(image_folder, f\"{fs_id}.png\")\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            response = run_qwen_inference(image_path, prompt)\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "    else:\n",
    "        response = \"Image not found\"\n",
    "\n",
    "    combined_hipped_results.append({\n",
    "        \"FS_DE_ID\": fs_id,\n",
    "        \"Ground Truth\": row[\"Combined_Hipped\"],\n",
    "        \"Prediction\": response\n",
    "    })\n",
    "\n",
    "combined_hipped_df = pd.DataFrame(combined_hipped_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bf4cf27-8315-47a5-9955-52c0b5f186db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Save directories\n",
    "csv_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions\"\n",
    "cm_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix\"\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… Improved Yes/No extractor\n",
    "def extract_yes_no(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    text = text.strip().lower()\n",
    "    if 'yes' in text and 'no' not in text:\n",
    "        return 'Y'\n",
    "    elif 'no' in text and 'yes' not in text:\n",
    "        return 'N'\n",
    "    return None  # ambiguous or both present\n",
    "\n",
    "def evaluate_and_save_binary(df, name, gt_col=\"Ground Truth\", pred_col=\"Prediction\"):\n",
    "    df[\"Prediction_Normalized\"] = df[pred_col].apply(extract_yes_no)\n",
    "\n",
    "    # Filter rows where prediction could be extracted\n",
    "    eval_df = df.dropna(subset=[\"Prediction_Normalized\"]).copy()\n",
    "\n",
    "    if eval_df.empty:\n",
    "        print(f\"\\nðŸš« Skipping '{name}' â€” no valid yes/no predictions found.\")\n",
    "        return\n",
    "\n",
    "    # Convert Y/N to 1/0\n",
    "    eval_df[\"Ground Truth Binary\"] = eval_df[gt_col].map({\"Y\": 1, \"N\": 0})\n",
    "    eval_df[\"Prediction Binary\"] = eval_df[\"Prediction_Normalized\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "    y_true = eval_df[\"Ground Truth Binary\"]\n",
    "    y_pred = eval_df[\"Prediction Binary\"]\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"No\", \"Yes\"], zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"], cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    cm_path = os.path.join(cm_output_dir, f\"{name.lower().replace(' ', '_')}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save predictions\n",
    "    export_df = eval_df[[\"FS_DE_ID\", \"Ground Truth Binary\", \"Prediction Binary\"]]\n",
    "    csv_path = os.path.join(csv_output_dir, f\"{name.lower().replace(' ', '_')}_predictions.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Saved to:\\n- {csv_path}\\n- {cm_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e465ce71-dff7-47ea-98d7-449fe0717039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMBINED HIPPED ROOF ===\n",
      "Accuracy: 0.7188755020080321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.65      0.82      0.73       114\n",
      "         Yes       0.81      0.63      0.71       135\n",
      "\n",
      "    accuracy                           0.72       249\n",
      "   macro avg       0.73      0.73      0.72       249\n",
      "weighted avg       0.74      0.72      0.72       249\n",
      "\n",
      "âœ… Saved to:\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions/combined_hipped_roof_predictions.csv\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix/combined_hipped_roof_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_save_binary(combined_hipped_df, \"Combined Hipped Roof\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb1072-6aee-48df-afb0-f6d63e7e8a49",
   "metadata": {},
   "source": [
    "### Inference Using Caption + QA Prompt (Hipped Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d448450-3795-4a43-a6a9-68d98542346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/249 [00:00<?, ?it/s]/mnt/home/gjenni/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [13:39<00:00,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "caption_prompt = \"Describe the roof shape of this building.\"\n",
    "qa_prompt_template = \"Based on your description, is the roof of hipped type? Answer only 'yes' or 'no'.\"\n",
    "\n",
    "caption_then_qa_results = []\n",
    "\n",
    "for idx, row in tqdm(df_combined.iterrows(), total=len(df_combined)):\n",
    "    fs_id = row[\"FS_DE_ID\"]\n",
    "    image_path = os.path.join(image_folder, f\"{fs_id}.png\")\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        response = \"Image not found\"\n",
    "        caption = None\n",
    "    else:\n",
    "        try:\n",
    "            # Step 1: Get roof description\n",
    "            caption = run_qwen_inference(image_path, caption_prompt)\n",
    "\n",
    "            # Step 2: Ask the model to classify based on its own description\n",
    "            follow_up_messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": caption},\n",
    "                        {\"type\": \"text\", \"text\": qa_prompt_template}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            follow_up_text = processor.apply_chat_template(\n",
    "                follow_up_messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            image_inputs, video_inputs = process_vision_info(follow_up_messages)\n",
    "\n",
    "            inputs = processor(\n",
    "                text=[follow_up_text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=64,\n",
    "                    do_sample=False,\n",
    "                    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                    pad_token_id=processor.tokenizer.pad_token_id\n",
    "                )\n",
    "\n",
    "            response = processor.batch_decode(\n",
    "                generated_ids[:, inputs['input_ids'].shape[1]:],\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            caption = None\n",
    "            response = f\"Error: {e}\"\n",
    "\n",
    "    caption_then_qa_results.append({\n",
    "        \"FS_DE_ID\": fs_id,\n",
    "        \"Ground Truth\": row[\"Combined_Hipped\"],\n",
    "        \"Caption\": caption,\n",
    "        \"Prediction\": response\n",
    "    })\n",
    "\n",
    "caption_then_qa_df = pd.DataFrame(caption_then_qa_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81f8b1c8-4670-4877-856a-f505939cccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Save directories\n",
    "csv_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions\"\n",
    "cm_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix\"\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… Improved Yes/No extractor\n",
    "def extract_yes_no(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    text = text.strip().lower()\n",
    "    if 'yes' in text and 'no' not in text:\n",
    "        return 'Y'\n",
    "    elif 'no' in text and 'yes' not in text:\n",
    "        return 'N'\n",
    "    return None  # ambiguous or both present\n",
    "\n",
    "def evaluate_and_save_binary(df, name, gt_col=\"Ground Truth\", pred_col=\"Prediction\"):\n",
    "    df[\"Prediction_Normalized\"] = df[pred_col].apply(extract_yes_no)\n",
    "\n",
    "    # Filter rows where prediction could be extracted\n",
    "    eval_df = df.dropna(subset=[\"Prediction_Normalized\"]).copy()\n",
    "\n",
    "    if eval_df.empty:\n",
    "        print(f\"\\nðŸš« Skipping '{name}' â€” no valid yes/no predictions found.\")\n",
    "        return\n",
    "\n",
    "    # Convert Y/N to 1/0\n",
    "    eval_df[\"Ground Truth Binary\"] = eval_df[gt_col].map({\"Y\": 1, \"N\": 0})\n",
    "    eval_df[\"Prediction Binary\"] = eval_df[\"Prediction_Normalized\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "    y_true = eval_df[\"Ground Truth Binary\"]\n",
    "    y_pred = eval_df[\"Prediction Binary\"]\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"No\", \"Yes\"], zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"], cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    cm_path = os.path.join(cm_output_dir, f\"{name.lower().replace(' ', '_')}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save predictions\n",
    "    export_df = eval_df[[\"FS_DE_ID\", \"Ground Truth Binary\", \"Prediction Binary\"]]\n",
    "    csv_path = os.path.join(csv_output_dir, f\"{name.lower().replace(' ', '_')}_predictions.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Saved to:\\n- {csv_path}\\n- {cm_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8bea89b-4b89-4cb1-89c3-728a88888520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMBINED HIPPED ROOF (CAPTION + QA) ===\n",
      "Accuracy: 0.4819277108433735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.47      1.00      0.64       114\n",
      "         Yes       1.00      0.04      0.09       135\n",
      "\n",
      "    accuracy                           0.48       249\n",
      "   macro avg       0.73      0.52      0.36       249\n",
      "weighted avg       0.76      0.48      0.34       249\n",
      "\n",
      "âœ… Saved to:\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions/combined_hipped_roof_(caption_+_qa)_predictions.csv\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix/combined_hipped_roof_(caption_+_qa)_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_save_binary(caption_then_qa_df, \"Combined Hipped Roof (Caption + QA)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f2dab9-6afa-4f39-aa96-691619f69431",
   "metadata": {},
   "source": [
    "### Inference for combined gable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8b90e2e-b54c-46e5-9506-f89e3e3684a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/249 [00:00<?, ?it/s]/mnt/home/gjenni/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:40<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# ðŸ“¢ Updated prompt for gable roofs\n",
    "prompt = \"Is the roof of the building of gabled type? Answer with only 'Yes' or 'No' by properly analyzing the image?\"\n",
    "\n",
    "combined_gable_results = []\n",
    "\n",
    "for idx, row in tqdm(df_combined.iterrows(), total=len(df_combined)):\n",
    "    fs_id = row[\"FS_DE_ID\"]\n",
    "    image_path = os.path.join(image_folder, f\"{fs_id}.png\")\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            response = run_qwen_inference(image_path, prompt)\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "    else:\n",
    "        response = \"Image not found\"\n",
    "\n",
    "    combined_gable_results.append({\n",
    "        \"FS_DE_ID\": fs_id,\n",
    "        \"Ground Truth\": row[\"Combined_Gable\"],\n",
    "        \"Prediction\": response\n",
    "    })\n",
    "\n",
    "combined_gable_df = pd.DataFrame(combined_gable_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50f794be-a729-4474-a783-7f423886d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Save directories\n",
    "csv_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions\"\n",
    "cm_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix\"\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… Improved Yes/No extractor\n",
    "def extract_yes_no(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    text = text.strip().lower()\n",
    "    if 'yes' in text and 'no' not in text:\n",
    "        return 'Y'\n",
    "    elif 'no' in text and 'yes' not in text:\n",
    "        return 'N'\n",
    "    return None  # ambiguous or both present\n",
    "\n",
    "def evaluate_and_save_binary(df, name, gt_col=\"Ground Truth\", pred_col=\"Prediction\"):\n",
    "    df[\"Prediction_Normalized\"] = df[pred_col].apply(extract_yes_no)\n",
    "\n",
    "    # Filter rows where prediction could be extracted\n",
    "    eval_df = df.dropna(subset=[\"Prediction_Normalized\"]).copy()\n",
    "\n",
    "    if eval_df.empty:\n",
    "        print(f\"\\nðŸš« Skipping '{name}' â€” no valid yes/no predictions found.\")\n",
    "        return\n",
    "\n",
    "    # Convert Y/N to 1/0\n",
    "    eval_df[\"Ground Truth Binary\"] = eval_df[gt_col].map({\"Y\": 1, \"N\": 0})\n",
    "    eval_df[\"Prediction Binary\"] = eval_df[\"Prediction_Normalized\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "    y_true = eval_df[\"Ground Truth Binary\"]\n",
    "    y_pred = eval_df[\"Prediction Binary\"]\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"No\", \"Yes\"], zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"], cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    cm_path = os.path.join(cm_output_dir, f\"{name.lower().replace(' ', '_')}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save predictions\n",
    "    export_df = eval_df[[\"FS_DE_ID\", \"Ground Truth Binary\", \"Prediction Binary\"]]\n",
    "    csv_path = os.path.join(csv_output_dir, f\"{name.lower().replace(' ', '_')}_predictions.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Saved to:\\n- {csv_path}\\n- {cm_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a987ad1d-0c29-41ec-9827-ac719242bfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMBINED GABLE ROOF ===\n",
      "Accuracy: 0.3413654618473896\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.31      0.18      0.23       135\n",
      "         Yes       0.35      0.54      0.43       114\n",
      "\n",
      "    accuracy                           0.34       249\n",
      "   macro avg       0.33      0.36      0.33       249\n",
      "weighted avg       0.33      0.34      0.32       249\n",
      "\n",
      "âœ… Saved to:\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions/combined_gable_roof_predictions.csv\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix/combined_gable_roof_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_save_binary(combined_gable_df, \"Combined Gable Roof\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1897c15-572e-488f-bca0-039ed8f8eaa5",
   "metadata": {},
   "source": [
    "### Inference Using Caption + QA Prompt (Gable Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1905905-ba75-44ca-b027-9ad0e71a452d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/249 [00:00<?, ?it/s]/mnt/home/gjenni/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [13:23<00:00,  3.23s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "caption_prompt = \"Describe the roof shape of this building.\"\n",
    "qa_prompt_template = \"Based on your description, is the roof of gable type? Answer only 'yes' or 'no'.\"\n",
    "\n",
    "caption_then_qa_results = []\n",
    "\n",
    "for idx, row in tqdm(df_combined.iterrows(), total=len(df_combined)):\n",
    "    fs_id = row[\"FS_DE_ID\"]\n",
    "    image_path = os.path.join(image_folder, f\"{fs_id}.png\")\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        response = \"Image not found\"\n",
    "        caption = None\n",
    "    else:\n",
    "        try:\n",
    "            # Step 1: Caption\n",
    "            caption = run_qwen_inference(image_path, caption_prompt)\n",
    "            \n",
    "            # Step 2: Feed caption back into the model with a follow-up question\n",
    "            follow_up_messages = [\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": caption},\n",
    "                    {\"type\": \"text\", \"text\": qa_prompt_template}\n",
    "                ]}\n",
    "            ]\n",
    "            follow_up_text = processor.apply_chat_template(follow_up_messages, tokenize=False, add_generation_prompt=True)\n",
    "            image_inputs, video_inputs = process_vision_info(follow_up_messages)\n",
    "\n",
    "            inputs = processor(\n",
    "                text=[follow_up_text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=64,\n",
    "                    do_sample=False,\n",
    "                    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                    pad_token_id=processor.tokenizer.pad_token_id\n",
    "                )\n",
    "\n",
    "            response = processor.batch_decode(\n",
    "                generated_ids[:, inputs['input_ids'].shape[1]:],\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            caption = None\n",
    "            response = f\"Error: {e}\"\n",
    "\n",
    "    caption_then_qa_results.append({\n",
    "        \"FS_DE_ID\": fs_id,\n",
    "        \"Ground Truth\": row[\"Combined_Gable\"],\n",
    "        \"Caption\": caption,\n",
    "        \"Prediction\": response\n",
    "    })\n",
    "\n",
    "caption_then_qa_df = pd.DataFrame(caption_then_qa_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c5bed8e-cf8a-433f-a174-46b1d2a3f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Save directories\n",
    "csv_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions\"\n",
    "cm_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix\"\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… Improved Yes/No extractor\n",
    "def extract_yes_no(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    text = text.strip().lower()\n",
    "    if 'yes' in text and 'no' not in text:\n",
    "        return 'Y'\n",
    "    elif 'no' in text and 'yes' not in text:\n",
    "        return 'N'\n",
    "    return None  # ambiguous or both present\n",
    "\n",
    "def evaluate_and_save_binary(df, name, gt_col=\"Ground Truth\", pred_col=\"Prediction\"):\n",
    "    df[\"Prediction_Normalized\"] = df[pred_col].apply(extract_yes_no)\n",
    "\n",
    "    # Filter rows where prediction could be extracted\n",
    "    eval_df = df.dropna(subset=[\"Prediction_Normalized\"]).copy()\n",
    "\n",
    "    if eval_df.empty:\n",
    "        print(f\"\\nðŸš« Skipping '{name}' â€” no valid yes/no predictions found.\")\n",
    "        return\n",
    "\n",
    "    # Convert Y/N to 1/0\n",
    "    eval_df[\"Ground Truth Binary\"] = eval_df[gt_col].map({\"Y\": 1, \"N\": 0})\n",
    "    eval_df[\"Prediction Binary\"] = eval_df[\"Prediction_Normalized\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "    y_true = eval_df[\"Ground Truth Binary\"]\n",
    "    y_pred = eval_df[\"Prediction Binary\"]\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"No\", \"Yes\"], zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"], cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    cm_path = os.path.join(cm_output_dir, f\"{name.lower().replace(' ', '_')}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save predictions\n",
    "    export_df = eval_df[[\"FS_DE_ID\", \"Ground Truth Binary\", \"Prediction Binary\"]]\n",
    "    csv_path = os.path.join(csv_output_dir, f\"{name.lower().replace(' ', '_')}_predictions.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Saved to:\\n- {csv_path}\\n- {cm_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "193c94d5-9f79-4712-898b-c89b24afe888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMBINED GABLE ROOF (CAPTION + QA) ===\n",
      "Accuracy: 0.41767068273092367\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.45      0.30      0.36       135\n",
      "         Yes       0.40      0.55      0.46       114\n",
      "\n",
      "    accuracy                           0.42       249\n",
      "   macro avg       0.42      0.43      0.41       249\n",
      "weighted avg       0.43      0.42      0.41       249\n",
      "\n",
      "âœ… Saved to:\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions/combined_gable_roof_(caption_+_qa)_predictions.csv\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix/combined_gable_roof_(caption_+_qa)_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_save_binary(caption_then_qa_df, \"Combined Gable Roof (Caption + QA)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccdc0c2-9a29-42b6-a925-38ad57b15422",
   "metadata": {},
   "source": [
    "## Roles as Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b868843-3015-4da3-8a66-d122a8d27206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qwen_inference(image_path, prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert in analyzing geospatial aerial images of residential buildings. Your expertise includes roof classification, roof volume analysis, and determining the utilization of roof spaces for living purposes. Provide detailed, accurate insights based on visual and structural features of roofs.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Vision input\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate only the new tokens\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Decode only the newly generated part\n",
    "    generated_text = processor.batch_decode(\n",
    "        generated_ids[:, inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )[0]\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6dd12cbe-24fa-4d7d-b37b-e430c24112e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:40<00:00,  6.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Prompt (use your best-performing one)\n",
    "prompt = \"Is the roof of the building of hipped type? Answer with only 'Yes' or 'No' by properly analyzing the image?\"\n",
    "\n",
    "combined_hipped_results_exp = []\n",
    "\n",
    "for idx, row in tqdm(df_combined.iterrows(), total=len(df_combined)):\n",
    "    fs_id = row[\"FS_DE_ID\"]\n",
    "    image_path = os.path.join(image_folder, f\"{fs_id}.png\")\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            response = run_qwen_inference(image_path, prompt)\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {e}\"\n",
    "    else:\n",
    "        response = \"Image not found\"\n",
    "\n",
    "    combined_hipped_results_exp.append({\n",
    "        \"FS_DE_ID\": fs_id,\n",
    "        \"Ground Truth\": row[\"Combined_Hipped\"],\n",
    "        \"Prediction\": response\n",
    "    })\n",
    "\n",
    "combined_hipped_df_exp = pd.DataFrame(combined_hipped_results_exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1453f4cc-798a-4323-96ef-9fc8906f75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Save directories\n",
    "csv_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions\"\n",
    "cm_output_dir = \"/mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix\"\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "os.makedirs(cm_output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… Improved Yes/No extractor\n",
    "def extract_yes_no(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    text = text.strip().lower()\n",
    "    if 'yes' in text and 'no' not in text:\n",
    "        return 'Y'\n",
    "    elif 'no' in text and 'yes' not in text:\n",
    "        return 'N'\n",
    "    return None  # ambiguous or both present\n",
    "\n",
    "def evaluate_and_save_binary(df, name, gt_col=\"Ground Truth\", pred_col=\"Prediction\"):\n",
    "    df[\"Prediction_Normalized\"] = df[pred_col].apply(extract_yes_no)\n",
    "\n",
    "    # Filter rows where prediction could be extracted\n",
    "    eval_df = df.dropna(subset=[\"Prediction_Normalized\"]).copy()\n",
    "\n",
    "    if eval_df.empty:\n",
    "        print(f\"\\nðŸš« Skipping '{name}' â€” no valid yes/no predictions found.\")\n",
    "        return\n",
    "\n",
    "    # Convert Y/N to 1/0\n",
    "    eval_df[\"Ground Truth Binary\"] = eval_df[gt_col].map({\"Y\": 1, \"N\": 0})\n",
    "    eval_df[\"Prediction Binary\"] = eval_df[\"Prediction_Normalized\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "    y_true = eval_df[\"Ground Truth Binary\"]\n",
    "    y_pred = eval_df[\"Prediction Binary\"]\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"No\", \"Yes\"], zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"], cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    cm_path = os.path.join(cm_output_dir, f\"{name.lower().replace(' ', '_')}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save predictions\n",
    "    export_df = eval_df[[\"FS_DE_ID\", \"Ground Truth Binary\", \"Prediction Binary\"]]\n",
    "    csv_path = os.path.join(csv_output_dir, f\"{name.lower().replace(' ', '_')}_predictions.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Saved to:\\n- {csv_path}\\n- {cm_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a07f4e0b-c020-47cd-aa86-b291c774456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMBINED HIPPED ROOF EXPERT ===\n",
      "Accuracy: 0.7068273092369478\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.63      0.89      0.73       114\n",
      "         Yes       0.85      0.56      0.67       135\n",
      "\n",
      "    accuracy                           0.71       249\n",
      "   macro avg       0.74      0.72      0.70       249\n",
      "weighted avg       0.75      0.71      0.70       249\n",
      "\n",
      "âœ… Saved to:\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Predictions/combined_hipped_roof_expert_predictions.csv\n",
      "- /mnt/data/oe215/env/guna/Qwen_2.5_VL/Qwen_7B/Confusion Matrix/combined_hipped_roof_expert_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_save_binary(combined_hipped_df_exp, \"Combined Hipped Roof Expert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa00cc-dce4-43a9-a31a-2dacc0ab289c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
