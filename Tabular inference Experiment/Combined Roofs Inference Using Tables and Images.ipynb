{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55665382-13f6-45f6-9750-d1f792eade5a",
   "metadata": {},
   "source": [
    "# Model Name: Qwen/Qwen2.5-VL-7B-Instruct\n",
    "# Dataset: Mixed Image Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ac85919-004a-420f-a363-5c47ddcdb179",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /mnt/home/gjenni/.local/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /mnt/home/gjenni/.local/lib/python3.12/site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in /mnt/home/gjenni/.local/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043b8437-1fa8-4c38-b483-8432f097915f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "CUDA Available: True\n",
      "GPU Name: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7caa8306-1fce-4589-a480-a2f0532b01d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/gjenni-tmpdir-MYds2U/pip-req-build-o_hwfqll\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/gjenni-tmpdir-MYds2U/pip-req-build-o_hwfqll\n",
      "  Resolved https://github.com/huggingface/transformers to commit a5c6172c81d69a6fa2c3b1340d72fc669b941dcd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /mnt/home/gjenni/.local/lib/python3.12/site-packages (1.4.0.dev0)\n",
      "Requirement already satisfied: filelock in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->transformers==4.52.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->transformers==4.52.0.dev0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->transformers==4.52.0.dev0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->transformers==4.52.0.dev0) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: qwen-vl-utils==0.0.8 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from qwen-vl-utils[decord]==0.0.8) (0.0.8)\n",
      "Requirement already satisfied: av in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (14.1.0)\n",
      "Requirement already satisfied: packaging in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (24.1)\n",
      "Requirement already satisfied: pillow in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (10.4.0)\n",
      "Requirement already satisfied: requests in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2.32.3)\n",
      "Requirement already satisfied: decord in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from qwen-vl-utils[decord]==0.0.8) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from decord->qwen-vl-utils[decord]==0.0.8) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers accelerate\n",
    "!pip install qwen-vl-utils[decord]==0.0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16949a33-b691-4d4d-9095-721bb2f8449c",
   "metadata": {},
   "source": [
    "### Load the model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80eb64af-0ca7-4cb1-8723-e8eaa40f9c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f76946b8244d4daebf222c65351bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Processor Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "# Load the model on the GPU\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "print(\"Model and Processor Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8883d0-1993-4321-9dd3-1e69aa407cf5",
   "metadata": {},
   "source": [
    "## AND Case\n",
    "### Single Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbc8f793-29e6-482b-ab7f-79728b612ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Tabular raw output:\n",
      " Given the following building attributes:\n",
      "- Building ID: 3581635.0\n",
      "- Maximum roof slope: 33.02212920656211 degrees\n",
      "- Maximum roof height: 5.129999999999996 meters\n",
      "\n",
      "If the maximum roof slope is greater than or equal to 30 degrees AND the maximum roof height is greater than or equal to 4 meters, predict that the attic is used as a living space.\n",
      "\n",
      "**At the beginning of your response, output only 'Yes' or 'No'. Do not repeat the input. Do not explain.** Yes\n",
      "ðŸ” Dormer raw output:\n",
      " system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Does the building roof have dormers? Answer with a yes or no.\n",
      " addCriterion\n",
      "No\n",
      "\n",
      "ðŸ“ Tabular condition (slope â‰¥ 30 AND height â‰¥ 4): yes â†’ 1\n",
      "ðŸªŸ Dormer detected: no â†’ 0\n",
      "ðŸ  Final Attic Usability Prediction: Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "# Load tabular data and one row\n",
    "csv_path = '/mnt/data/oe215/env/guna/tabular_image_inference/data/tabular_data/mixed_table/attic_GT_IMG_AND_and_GT_IMG_OR_Labels.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "row = df.iloc[375]\n",
    "\n",
    "# Load corresponding image\n",
    "image_id = str(int(row['gid'])) + \".png\"\n",
    "image_path = f\"/mnt/data/oe215/env/guna/tabular_image_inference/data/image_data/mixed/{image_id}\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# âœ… Final answer extractor using last YES/NO in response\n",
    "def extract_final_yes_no(text):\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove punctuation and formatting (**)\n",
    "    words = text.split()\n",
    "    for word in reversed(words):\n",
    "        if word == \"yes\":\n",
    "            return \"yes\"\n",
    "        elif word == \"no\":\n",
    "            return \"no\"\n",
    "    return \"unknown\"\n",
    "\n",
    "# Convert answer to binary\n",
    "def parse_binary_from_answer(answer):\n",
    "    answer = answer.strip().lower()\n",
    "    if answer == \"yes\":\n",
    "        return 1\n",
    "    elif answer == \"no\":\n",
    "        return 0\n",
    "    return -1\n",
    "\n",
    "# Tabular prompt function\n",
    "def predict_from_tabular(row, processor, model):\n",
    "    prompt = (\n",
    "        f\"Given the following building attributes:\\n\"\n",
    "        f\"- Building ID: {row['gid']}\\n\"\n",
    "        f\"- Maximum roof slope: {row['slope_max']} degrees\\n\"\n",
    "        f\"- Maximum roof height: {row['roof_height_max']} meters\\n\\n\"\n",
    "        \"If the maximum roof slope is greater than or equal to 30 degrees AND the maximum roof height is greater than or equal to 4 meters, \"\n",
    "        \"predict that the attic is used as a living space.\\n\\n\"\n",
    "        \"**At the beginning of your response, output only 'Yes' or 'No'. Do not repeat the input. Do not explain.**\"\n",
    "    )\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    decoded = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "    print(\"ðŸ” Tabular raw output:\\n\", decoded)\n",
    "    return extract_final_yes_no(decoded)\n",
    "\n",
    "# Dormer detection prompt\n",
    "def detect_dormer(image, processor, model):\n",
    "    prompt = \"Does the building roof have dormers? Answer with a yes or no.\"\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "    prompt_str = processor.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = processor(text=prompt_str, images=image, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    decoded = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "    print(\"ðŸ” Dormer raw output:\\n\", decoded)\n",
    "    return extract_final_yes_no(decoded)\n",
    "\n",
    "# Run both inference stages\n",
    "tabular_result = predict_from_tabular(row, processor, model)\n",
    "dormer_result = detect_dormer(image, processor, model)\n",
    "\n",
    "# Convert to binary\n",
    "tabular_binary = parse_binary_from_answer(tabular_result)\n",
    "dormer_binary = parse_binary_from_answer(dormer_result)\n",
    "\n",
    "# Final prediction logic\n",
    "if tabular_binary == 1 or dormer_binary == 1:\n",
    "    final_prediction = 1\n",
    "elif tabular_binary == 0 and dormer_binary == 0:\n",
    "    final_prediction = 0\n",
    "else:\n",
    "    final_prediction = -1  # unknown\n",
    "\n",
    "# Output final results\n",
    "print(f\"\\nðŸ“ Tabular condition (slope â‰¥ 30 AND height â‰¥ 4): {tabular_result} â†’ {tabular_binary}\")\n",
    "print(f\"ðŸªŸ Dormer detected: {dormer_result} â†’ {dormer_binary}\")\n",
    "print(f\"ðŸ  Final Attic Usability Prediction: {'Yes' if final_prediction == 1 else 'No' if final_prediction == 0 else 'Unknown'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde0fe4-1ab3-4f86-ad15-6e6b53905858",
   "metadata": {},
   "source": [
    "### Image Inference Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4a70d-ffc3-42ee-9e1e-8d2682719f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from modelscope import Qwen2_5_VLForConditionalGeneration\n",
    "from transformers import AutoProcessor\n",
    "import re\n",
    "\n",
    "\n",
    "# Load a test image\n",
    "image_path = \"/mnt/data/oe215/env/guna/tabular_image_inference/data/image_data/mixed/3581560.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Function to extract yes/no from final words\n",
    "def extract_final_yes_no(text):\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove punctuation like ** or .\n",
    "    words = text.split()\n",
    "    for word in reversed(words):\n",
    "        if word == \"yes\":\n",
    "            return \"yes\"\n",
    "        elif word == \"no\":\n",
    "            return \"no\"\n",
    "    return \"unknown\"\n",
    "\n",
    "# Function to run dormer inference on a single image\n",
    "def detect_dormer_single_image(image, processor, model):\n",
    "    prompt = \"D?\"\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "    prompt_str = processor.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = processor(text=prompt_str, images=image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "    decoded = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "    print(\"ðŸ” Model raw output:\\n\", decoded)\n",
    "\n",
    "    return extract_final_yes_no(decoded)\n",
    "\n",
    "# Run single image inference\n",
    "dormer_prediction = detect_dormer_single_image(image, processor, model)\n",
    "print(f\"\\nðŸªŸ Dormer Prediction: {dormer_prediction.capitalize()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "171f74a0-29f8-4fcf-ab36-28580ddb5f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¸ General Image Description:\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Please describe the contents of this aerial image.\n",
      "Focus on the building structure, roof shape, and any notable features such as dormers, skylights, chimneys, or surrounding environment.\n",
      "Keep the response short and factual.\n",
      " addCriterion\n",
      "The image shows an aerial view of a large, rectangular building with a flat roof. The building has several sections, possibly indicating different wings or functional areas. There are no visible dormers, skylights, or chimneys in the image. The surrounding area includes greenery, suggesting a landscaped environment, and there appears to be a parking lot adjacent to the building.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load your test aerial image\n",
    "image_path = \"/mnt/data/oe215/env/guna/tabular_image_inference/data/image_data/mixed/3581585.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Prompt: General image description\n",
    "def create_general_description_prompt():\n",
    "    return (\n",
    "        \"Please describe the contents of this aerial image.\\n\"\n",
    "        \"Focus on the building structure, roof shape, and any notable features such as dormers, skylights, chimneys, or surrounding environment.\\n\"\n",
    "        \"Keep the response short and factual.\"\n",
    "    )\n",
    "\n",
    "# Function to run inference\n",
    "def infer_general_description(image, processor, model):\n",
    "    prompt = create_general_description_prompt()\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "    prompt_str = processor.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    inputs = processor(text=prompt_str, images=image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "    decoded = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "    return decoded\n",
    "\n",
    "# Run the test\n",
    "description = infer_general_description(image, processor, model)\n",
    "print(\"ðŸ“¸ General Image Description:\\n\")\n",
    "print(description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217979da-5fa1-42b9-b24e-17785c412d8e",
   "metadata": {},
   "source": [
    "### Batch Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e4b066-82f3-4415-8ab4-7ee1963a4d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 745/745 [01:42<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved to: /mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions/mixed_img_predictions_AND.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load data\n",
    "csv_path = '/mnt/data/oe215/env/guna/tabular_image_inference/data/tabular_data/mixed_table/attic_GT_IMG_AND_and_GT_IMG_OR_Labels.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Image directory\n",
    "image_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/data/image_data/mixed/'\n",
    "\n",
    "# Prediction helpers\n",
    "def extract_final_yes_no(text):\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    words = text.split()\n",
    "    for word in reversed(words):\n",
    "        if word == \"yes\":\n",
    "            return \"yes\"\n",
    "        elif word == \"no\":\n",
    "            return \"no\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def parse_binary_from_answer(answer):\n",
    "    return {\"yes\": 1, \"no\": 0}.get(answer.strip().lower(), -1)\n",
    "\n",
    "def predict_from_tabular(row):\n",
    "    prompt = (\n",
    "        f\"Given the following building attributes:\\n\"\n",
    "        f\"- Building ID: {row['gid']}\\n\"\n",
    "        f\"- Maximum roof slope: {row['slope_max']} degrees\\n\"\n",
    "        f\"- Maximum roof height: {row['roof_height_max']} meters\\n\\n\"\n",
    "        \"If the maximum roof slope is greater than or equal to 30 degrees AND the maximum roof height is greater than or equal to 4 meters, \"\n",
    "        \"predict that the attic is used as a living space.\\n\\n\"\n",
    "        \"**At the beginning of your response, output only 'Yes' or 'No'. Do not repeat the input. Do not explain.**\"\n",
    "    )\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    decoded = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "    return extract_final_yes_no(decoded)\n",
    "\n",
    "def detect_dormer(image):\n",
    "    prompt = \"Does the building roof have dormers? Answer with a yes or no.\"\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "    prompt_str = processor.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = processor(text=prompt_str, images=image, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    decoded = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "    return extract_final_yes_no(decoded)\n",
    "\n",
    "# Run batch\n",
    "predictions = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    try:\n",
    "        image_id = f\"{int(row['gid'])}.png\"\n",
    "        image_path = os.path.join(image_dir, image_id)\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            predictions.append(-1)\n",
    "            continue\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        tabular = predict_from_tabular(row)\n",
    "        dormer = detect_dormer(image)\n",
    "\n",
    "        tab = parse_binary_from_answer(tabular)\n",
    "        dor = parse_binary_from_answer(dormer)\n",
    "\n",
    "        if tab == 1 or dor == 1:\n",
    "            predictions.append(1)\n",
    "        elif tab == 0 and dor == 0:\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "\n",
    "    except Exception as e:\n",
    "        predictions.append(-1)\n",
    "\n",
    "# Save predictions\n",
    "df[\"pred_img_AND\"] = predictions\n",
    "output_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, 'mixed_img_predictions_AND.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"âœ… Saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519bdefb-8a4e-41d5-bce2-69af1b18a7cd",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b9c1422-bfe7-4b16-a41c-4d4734d5bcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Accuracy: 0.6899\n",
      "\n",
      "ðŸ§® Confusion Matrix (Numbers):\n",
      "[[271 110]\n",
      " [121 243]]\n",
      "\n",
      "ðŸ“„ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Used       0.69      0.71      0.70       381\n",
      "        Used       0.69      0.67      0.68       364\n",
      "\n",
      "    accuracy                           0.69       745\n",
      "   macro avg       0.69      0.69      0.69       745\n",
      "weighted avg       0.69      0.69      0.69       745\n",
      "\n",
      "\n",
      "âœ… Confusion matrix plot saved successfully at: /mnt/data/oe215/env/guna/tabular_image_inference/outputs/evaluations/confusion_matrix/conf_mat_img_mixed_AND.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/gjenni-tmpdir-oi4jse/ipykernel_1021632/2025279655.py:40: UserWarning: Glyph 127968 (\\N{HOUSE BUILDING}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/gjenni-tmpdir-oi4jse/ipykernel_1021632/2025279655.py:46: UserWarning: Glyph 127968 (\\N{HOUSE BUILDING}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load the predictions CSV\n",
    "df = pd.read_csv('/mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions/mixed_img_predictions_AND.csv')\n",
    "\n",
    "# 2. Define ground truth and model predictions\n",
    "y_true = df['GT_IMG_AND']\n",
    "y_pred = df['pred_img_AND']\n",
    "\n",
    "# 3. Calculate Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nâœ… Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 4. Show Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nðŸ§® Confusion Matrix (Numbers):\")\n",
    "print(cm)\n",
    "\n",
    "# 5. Classification Report\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Not Used\", \"Used\"])\n",
    "print(\"\\nðŸ“„ Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# 4. Create figure\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "\n",
    "# 5. Plot Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Predicted Not Used\", \"Predicted Used\"],\n",
    "            yticklabels=[\"Actual Not Used\", \"Actual Used\"],\n",
    "            ax=ax)\n",
    "ax.set_title('ðŸ  Confusion Matrix for Attic Living Space Prediction')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "\n",
    "# 6. Tight Layout and Save\n",
    "plt.tight_layout()\n",
    "\n",
    "output_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/outputs/evaluations/confusion_matrix'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(output_dir, 'conf_mat_img_mixed_AND.png')\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# 7. Close the figure AFTER saving\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nâœ… Confusion matrix plot saved successfully at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc7d46-1786-45b8-8b24-22f30111c8d4",
   "metadata": {},
   "source": [
    "## OR Case\n",
    "### Single Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b8b70e-3bd2-4852-9329-3b93621156cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Tabular raw output:\n",
      " Given the following building attributes:\n",
      "- Building ID: 3581585.0\n",
      "- Maximum roof slope: 27.147534846875136 degrees\n",
      "- Maximum roof height: 1.8739999999999952 meters\n",
      "\n",
      "If the maximum roof slope is greater than or equal to 30 degrees OR the maximum roof height is greater than or equal to 4 meters, predict that the attic is used as a living space.\n",
      "\n",
      "**At the beginning of your response, output only 'Yes' or 'No'. Do not repeat the input. Do not explain.** No\n",
      "ðŸ” Dormer raw output:\n",
      " system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Does the building roof have dormers? Answer with a yes or no.\n",
      " addCriterion\n",
      "No\n",
      "\n",
      "ðŸ“ Tabular condition (slope â‰¥ 30 AND height â‰¥ 4): no â†’ 0\n",
      "ðŸªŸ Dormer detected: no â†’ 0\n",
      "ðŸ  Final Attic Usability Prediction: No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "# Load tabular data and one row\n",
    "csv_path = '/mnt/data/oe215/env/guna/tabular_image_inference/data/tabular_data/mixed_table/attic_GT_IMG_AND_and_GT_IMG_OR_Labels.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "row = df.iloc[1]\n",
    "\n",
    "# Load corresponding image\n",
    "image_id = str(int(row['gid'])) + \".png\"\n",
    "image_path = f\"/mnt/data/oe215/env/guna/tabular_image_inference/data/image_data/mixed/{image_id}\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# âœ… Final answer extractor using last YES/NO in response\n",
    "def extract_final_yes_no(text):\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove punctuation and formatting (**)\n",
    "    words = text.split()\n",
    "    for word in reversed(words):\n",
    "        if word == \"yes\":\n",
    "            return \"yes\"\n",
    "        elif word == \"no\":\n",
    "            return \"no\"\n",
    "    return \"unknown\"\n",
    "\n",
    "# Convert answer to binary\n",
    "def parse_binary_from_answer(answer):\n",
    "    answer = answer.strip().lower()\n",
    "    if answer == \"yes\":\n",
    "        return 1\n",
    "    elif answer == \"no\":\n",
    "        return 0\n",
    "    return -1\n",
    "\n",
    "# Tabular prompt function\n",
    "def predict_from_tabular(row, processor, model):\n",
    "    prompt = (\n",
    "        f\"Given the following building attributes:\\n\"\n",
    "        f\"- Building ID: {row['gid']}\\n\"\n",
    "        f\"- Maximum roof slope: {row['slope_max']} degrees\\n\"\n",
    "        f\"- Maximum roof height: {row['roof_height_max']} meters\\n\\n\"\n",
    "        \"If the maximum roof slope is greater than or equal to 30 degrees OR the maximum roof height is greater than or equal to 4 meters, \"\n",
    "        \"predict that the attic is used as a living space.\\n\\n\"\n",
    "        \"**At the beginning of your response, output only 'Yes' or 'No'. Do not repeat the input. Do not explain.**\"\n",
    "    )\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    decoded = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "    print(\"ðŸ” Tabular raw output:\\n\", decoded)\n",
    "    return extract_final_yes_no(decoded)\n",
    "\n",
    "# Dormer detection prompt\n",
    "def detect_dormer(image, processor, model):\n",
    "    prompt = \"Does the building roof have dormers? Answer with a yes or no.\"\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "    prompt_str = processor.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = processor(text=prompt_str, images=image, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    decoded = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "    print(\"ðŸ” Dormer raw output:\\n\", decoded)\n",
    "    return extract_final_yes_no(decoded)\n",
    "\n",
    "# Run both inference stages\n",
    "tabular_result = predict_from_tabular(row, processor, model)\n",
    "dormer_result = detect_dormer(image, processor, model)\n",
    "\n",
    "# Convert to binary\n",
    "tabular_binary = parse_binary_from_answer(tabular_result)\n",
    "dormer_binary = parse_binary_from_answer(dormer_result)\n",
    "\n",
    "# Final prediction logic\n",
    "if tabular_binary == 1 or dormer_binary == 1:\n",
    "    final_prediction = 1\n",
    "elif tabular_binary == 0 and dormer_binary == 0:\n",
    "    final_prediction = 0\n",
    "else:\n",
    "    final_prediction = -1  # unknown\n",
    "\n",
    "# Output final results\n",
    "print(f\"\\nðŸ“ Tabular condition (slope â‰¥ 30 AND height â‰¥ 4): {tabular_result} â†’ {tabular_binary}\")\n",
    "print(f\"ðŸªŸ Dormer detected: {dormer_result} â†’ {dormer_binary}\")\n",
    "print(f\"ðŸ  Final Attic Usability Prediction: {'Yes' if final_prediction == 1 else 'No' if final_prediction == 0 else 'Unknown'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0283ef-d035-4fa2-9fee-dfaf1bf4c366",
   "metadata": {},
   "source": [
    "### Batch Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c5bc04-dd10-40be-b698-b7317809cefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 745/745 [01:42<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved to: /mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions/mixed_img_predictions_OR.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load data\n",
    "csv_path = '/mnt/data/oe215/env/guna/tabular_image_inference/data/tabular_data/mixed_table/attic_GT_IMG_AND_and_GT_IMG_OR_Labels.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Image directory\n",
    "image_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/data/image_data/mixed/'\n",
    "\n",
    "# Prediction helpers\n",
    "def extract_final_yes_no(text):\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    words = text.split()\n",
    "    for word in reversed(words):\n",
    "        if word == \"yes\":\n",
    "            return \"yes\"\n",
    "        elif word == \"no\":\n",
    "            return \"no\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def parse_binary_from_answer(answer):\n",
    "    return {\"yes\": 1, \"no\": 0}.get(answer.strip().lower(), -1)\n",
    "\n",
    "def predict_from_tabular(row):\n",
    "    prompt = (\n",
    "        f\"Given the following building attributes:\\n\"\n",
    "        f\"- Building ID: {row['gid']}\\n\"\n",
    "        f\"- Maximum roof slope: {row['slope_max']} degrees\\n\"\n",
    "        f\"- Maximum roof height: {row['roof_height_max']} meters\\n\\n\"\n",
    "        \"If the maximum roof slope is greater than or equal to 30 degrees OR the maximum roof height is greater than or equal to 4 meters, \"\n",
    "        \"predict that the attic is used as a living space.\\n\\n\"\n",
    "        \"**At the beginning of your response, output only 'Yes' or 'No'. Do not repeat the input. Do not explain.**\"\n",
    "    )\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    decoded = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "    return extract_final_yes_no(decoded)\n",
    "\n",
    "def detect_dormer(image):\n",
    "    prompt = \"Does the building roof have dormers? Answer with a yes or no.\"\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "    prompt_str = processor.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = processor(text=prompt_str, images=image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    decoded = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "    return extract_final_yes_no(decoded)\n",
    "\n",
    "# Run batch\n",
    "predictions = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    try:\n",
    "        image_id = f\"{int(row['gid'])}.png\"\n",
    "        image_path = os.path.join(image_dir, image_id)\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            predictions.append(-1)\n",
    "            continue\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        tabular = predict_from_tabular(row)\n",
    "        dormer = detect_dormer(image)\n",
    "\n",
    "        tab = parse_binary_from_answer(tabular)\n",
    "        dor = parse_binary_from_answer(dormer)\n",
    "\n",
    "        if tab == 1 or dor == 1:\n",
    "            predictions.append(1)\n",
    "        elif tab == 0 and dor == 0:\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "\n",
    "    except Exception as e:\n",
    "        predictions.append(-1)\n",
    "\n",
    "# Save predictions\n",
    "df[\"pred_img_OR\"] = predictions\n",
    "output_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, 'mixed_img_predictions_OR.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"âœ… Saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc77479c-98da-4dc2-b9df-465a4d994c02",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f48fb15-2838-47b9-b29d-864e85bca959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Accuracy: 0.6711\n",
      "\n",
      "ðŸ§® Confusion Matrix (Numbers):\n",
      "[[194  83]\n",
      " [162 306]]\n",
      "\n",
      "ðŸ“„ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Used       0.54      0.70      0.61       277\n",
      "        Used       0.79      0.65      0.71       468\n",
      "\n",
      "    accuracy                           0.67       745\n",
      "   macro avg       0.67      0.68      0.66       745\n",
      "weighted avg       0.70      0.67      0.68       745\n",
      "\n",
      "\n",
      "âœ… Confusion matrix plot saved successfully at: /mnt/data/oe215/env/guna/tabular_image_inference/outputs/evaluations/confusion_matrix/conf_mat_img_mixed_OR.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/gjenni-tmpdir-oi4jse/ipykernel_1021632/308870818.py:40: UserWarning: Glyph 127968 (\\N{HOUSE BUILDING}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/gjenni-tmpdir-oi4jse/ipykernel_1021632/308870818.py:46: UserWarning: Glyph 127968 (\\N{HOUSE BUILDING}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load the predictions CSV\n",
    "df = pd.read_csv('/mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions/mixed_img_predictions_OR.csv')\n",
    "\n",
    "# 2. Define ground truth and model predictions\n",
    "y_true = df['GT_IMG_OR']\n",
    "y_pred = df['pred_img_OR']\n",
    "\n",
    "# 3. Calculate Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nâœ… Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 4. Show Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nðŸ§® Confusion Matrix (Numbers):\")\n",
    "print(cm)\n",
    "\n",
    "# 5. Classification Report\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Not Used\", \"Used\"])\n",
    "print(\"\\nðŸ“„ Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# 4. Create figure\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "\n",
    "# 5. Plot Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Predicted Not Used\", \"Predicted Used\"],\n",
    "            yticklabels=[\"Actual Not Used\", \"Actual Used\"],\n",
    "            ax=ax)\n",
    "ax.set_title('ðŸ  Confusion Matrix for Attic Living Space Prediction')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "\n",
    "# 6. Tight Layout and Save\n",
    "plt.tight_layout()\n",
    "\n",
    "output_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/outputs/evaluations/confusion_matrix'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(output_dir, 'conf_mat_img_mixed_OR.png')\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# 7. Close the figure AFTER saving\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nâœ… Confusion matrix plot saved successfully at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08d436-1517-4ded-84da-bc6c2e5d42f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
