{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433f3340-e5d2-45ae-b2d2-2ee31b076a63",
   "metadata": {},
   "source": [
    "# Model Name: Qwen/Qwen2.5-VL-7B-Instruct\n",
    "# Dataset: Gable Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e53266a-3f61-4edb-89ed-0da5c1c80ab4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  4 14:37:29 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   43C    P0             47W /  300W |       0MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd41609d-c49d-41ee-be94-b2ca1b29898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /mnt/home/gjenni/.local/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /mnt/home/gjenni/.local/lib/python3.12/site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in /mnt/home/gjenni/.local/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3221162-65a5-4a51-a8f6-ec531ed64236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "CUDA Available: True\n",
      "GPU Name: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0337e10a-c0b4-43ce-801e-d2fa3e2cdd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/gjenni-tmpdir-u7tWn9/pip-req-build-qf3bm4qq\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/gjenni-tmpdir-u7tWn9/pip-req-build-qf3bm4qq\n",
      "  Resolved https://github.com/huggingface/transformers to commit 397a5ede33863d6f7137c771a68d40036cac0396\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /mnt/home/gjenni/.local/lib/python3.12/site-packages (1.4.0.dev0)\n",
      "Requirement already satisfied: filelock in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers==4.52.0.dev0)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->transformers==4.52.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->transformers==4.52.0.dev0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->transformers==4.52.0.dev0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->transformers==4.52.0.dev0) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.52.0.dev0-py3-none-any.whl size=11457285 sha256=161e57e11df07fc0766a91ac0109eae79dc0fdfa4564d2192b7a4246da2c3883\n",
      "  Stored in directory: /tmp/gjenni-tmpdir-u7tWn9/pip-ephem-wheel-cache-a_zpd7gi/wheels/49/a7/50/c9fdabbf10e51bb1256adb0c1a587fedd7184f5bad28d47fe3\n",
      "Successfully built transformers\n",
      "Installing collected packages: huggingface-hub, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.26.5\n",
      "    Uninstalling huggingface-hub-0.26.5:\n",
      "      Successfully uninstalled huggingface-hub-0.26.5\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.0.dev0\n",
      "    Uninstalling transformers-4.51.0.dev0:\n",
      "      Successfully uninstalled transformers-4.51.0.dev0\n",
      "Successfully installed huggingface-hub-0.30.2 transformers-4.52.0.dev0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: qwen-vl-utils==0.0.8 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from qwen-vl-utils[decord]==0.0.8) (0.0.8)\n",
      "Requirement already satisfied: av in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (14.1.0)\n",
      "Requirement already satisfied: packaging in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (24.1)\n",
      "Requirement already satisfied: pillow in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (10.4.0)\n",
      "Requirement already satisfied: requests in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2.32.3)\n",
      "Requirement already satisfied: decord in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from qwen-vl-utils[decord]==0.0.8) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /mnt/home/gjenni/.local/lib/python3.12/site-packages (from decord->qwen-vl-utils[decord]==0.0.8) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/apps/modules/conda/python3.12.0/lib/python3.12/site-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers accelerate\n",
    "!pip install qwen-vl-utils[decord]==0.0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad09be0-5d6e-4fa2-8f18-7252a55c68e6",
   "metadata": {},
   "source": [
    "### Load the model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a682bb-5253-4ec0-bed4-cca552ba964b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6df779897a44a918b3740fa3e2456be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Processor Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "# Load the model on the GPU\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "print(\"Model and Processor Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e6c7d-66e0-4eb6-a7e8-0fbe2bedde5b",
   "metadata": {},
   "source": [
    "## AND Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260ba974-76e1-4d2b-bbe7-cfec45bebe73",
   "metadata": {},
   "source": [
    "### Single Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c05348b-ff16-4fbb-bbb9-cb1db5ebcd17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompt:\n",
      "\n",
      "Given the following building attributes:\n",
      "- Building ID: 3581560.0\n",
      "- Maximum roof slope: 51.01464222107296 degrees\n",
      "- Maximum roof height: 3.984000000000009 meters\n",
      "\n",
      "If the maximum roof slope is greater than or equal to 30 degrees AND the maximum roof height is greater than or equal to 4 meters, predict that the attic is used as a living space.\n",
      "\n",
      "**At the beginning of your response, output only 'Yes' or 'No' on a new line. Do not repeat the input. Do not explain.**\n",
      "\n",
      "Full Model Output:\n",
      "\n",
      "Given the following building attributes:\n",
      "- Building ID: 3581560.0\n",
      "- Maximum roof slope: 51.01464222107296 degrees\n",
      "- Maximum roof height: 3.984000000000009 meters\n",
      "\n",
      "If the maximum roof slope is greater than or equal to 30 degrees AND the maximum roof height is greater than or equal to 4 meters, predict that the attic is used as a living space.\n",
      "\n",
      "**At the beginning of your response, output only 'Yes' or 'No' on a new line. Do not repeat the input. Do not explain.** No\n",
      "Model final prediction: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. Load the CSV\n",
    "df = pd.read_csv('/mnt/data/oe215/env/guna/tabular_image_inference/data/tabular_data/gable_table/attic_ground_truths_gable_3100.csv')\n",
    "\n",
    "# 2. Select the first row\n",
    "row = df.iloc[0]\n",
    "\n",
    "# 3. Create the prompt\n",
    "def create_improved_prompt(row):\n",
    "    return (\n",
    "        f\"Given the following building attributes:\\n\"\n",
    "        f\"- Building ID: {row['gid']}\\n\"\n",
    "        f\"- Maximum roof slope: {row['slope_max']} degrees\\n\"\n",
    "        f\"- Maximum roof height: {row['roof_height_max']} meters\\n\\n\"\n",
    "        \"If the maximum roof slope is greater than or equal to 30 degrees AND the maximum roof height is greater than or equal to 4 meters, \"\n",
    "        \"predict that the attic is used as a living space.\\n\\n\"\n",
    "        \"**At the beginning of your response, output only 'Yes' or 'No' on a new line. Do not repeat the input. Do not explain.**\"\n",
    "    )\n",
    "\n",
    "\n",
    "prompt = create_improved_prompt(row)\n",
    "\n",
    "\n",
    "print(\"Generated Prompt:\\n\")\n",
    "print(prompt)\n",
    "\n",
    "# 4. Perform inference\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "\n",
    "\n",
    "# 5. Decode and just print\n",
    "prediction = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "prediction = prediction.strip()\n",
    "\n",
    "# Print full model output to debug\n",
    "print(\"\\nFull Model Output:\\n\")\n",
    "print(prediction)\n",
    "\n",
    "\n",
    "# After model prediction\n",
    "final_text = prediction.strip().lower()\n",
    "\n",
    "if \" yes\" in final_text or final_text.endswith('yes'):\n",
    "    pred_value = 1\n",
    "elif \" no\" in final_text or final_text.endswith('no'):\n",
    "    pred_value = 0\n",
    "else:\n",
    "    pred_value = -1\n",
    "\n",
    "print(\"Model final prediction:\", pred_value)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a2a946-c18d-45d5-8636-5d7ea97e3eb7",
   "metadata": {},
   "source": [
    "### Batch Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf60e49a-1a47-4538-8065-8cd843836a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference on all rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [01:04<00:00,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All predictions saved successfully to: /mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions/gable_predictions_AND.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "# 1. Load the CSV\n",
    "df = pd.read_csv('/mnt/data/oe215/env/guna/tabular_image_inference/data/tabular_data/gable_table/attic_ground_truths_gable_3100.csv')\n",
    "\n",
    "# 2. Create the improved prompt function\n",
    "def create_improved_prompt(row):\n",
    "    return (\n",
    "        f\"Given the following building attributes:\\n\"\n",
    "        f\"- Building ID: {row['gid']}\\n\"\n",
    "        f\"- Maximum roof slope: {row['slope_max']} degrees\\n\"\n",
    "        f\"- Maximum roof height: {row['roof_height_max']} meters\\n\\n\"\n",
    "        \"If the maximum roof slope is greater than or equal to 30 degrees AND the maximum roof height is greater than or equal to 4 meters, \"\n",
    "        \"predict that the attic is used as a living space.\\n\\n\"\n",
    "        \"**At the beginning of your response, output only 'Yes' or 'No' on a new line. Do not repeat the input. Do not explain.**\"\n",
    "    )\n",
    "\n",
    "# 3. Function to perform prediction for a given prompt\n",
    "def predict_attic_use(prompt):\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "    prediction = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    prediction = prediction.strip()\n",
    "\n",
    "    final_text = prediction.strip().lower()\n",
    "\n",
    "    if \" yes\" in final_text or final_text.endswith('yes'):\n",
    "        return 1\n",
    "    elif \" no\" in final_text or final_text.endswith('no'):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1  # unexpected case\n",
    "\n",
    "# 4. Inference for all rows\n",
    "predictions = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Running inference on all rows\"):\n",
    "    prompt = create_improved_prompt(row)\n",
    "    pred = predict_attic_use(prompt)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# 5. Add predictions to dataframe\n",
    "df['pred_AND'] = predictions\n",
    "\n",
    "# 6. Save the updated dataframe\n",
    "output_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, 'gable_predictions_AND.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… All predictions saved successfully to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7054530a-c09d-4ad3-bbea-4a4b805b7c8b",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca557d36-5394-4f67-8eb6-972b67491ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Accuracy: 1.0000\n",
      "\n",
      "ðŸ§® Confusion Matrix (Numbers):\n",
      "[[282   0]\n",
      " [  0  93]]\n",
      "\n",
      "ðŸ“„ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Used       1.00      1.00      1.00       282\n",
      "        Used       1.00      1.00      1.00        93\n",
      "\n",
      "    accuracy                           1.00       375\n",
      "   macro avg       1.00      1.00      1.00       375\n",
      "weighted avg       1.00      1.00      1.00       375\n",
      "\n",
      "\n",
      "âœ… Confusion matrix plot saved successfully at: /mnt/data/oe215/env/guna/tabular_image_inference/outputs/evaluations/confusion_matrix/conf_mat_gable_AND.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/gjenni-tmpdir-gmWwWw/ipykernel_3476555/2219299386.py:40: UserWarning: Glyph 127968 (\\N{HOUSE BUILDING}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/gjenni-tmpdir-gmWwWw/ipykernel_3476555/2219299386.py:46: UserWarning: Glyph 127968 (\\N{HOUSE BUILDING}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load the predictions CSV\n",
    "df = pd.read_csv('/mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions/gable_predictions_AND.csv')\n",
    "\n",
    "# 2. Define ground truth and model predictions\n",
    "y_true = df['GT_AND']\n",
    "y_pred = df['pred_AND']\n",
    "\n",
    "# 3. Calculate Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nâœ… Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 4. Show Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nðŸ§® Confusion Matrix (Numbers):\")\n",
    "print(cm)\n",
    "\n",
    "# 5. Classification Report\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Not Used\", \"Used\"])\n",
    "print(\"\\nðŸ“„ Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# 4. Create figure\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "\n",
    "# 5. Plot Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Predicted Not Used\", \"Predicted Used\"],\n",
    "            yticklabels=[\"Actual Not Used\", \"Actual Used\"],\n",
    "            ax=ax)\n",
    "ax.set_title('ðŸ  Confusion Matrix for Attic Living Space Prediction')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "\n",
    "# 6. Tight Layout and Save\n",
    "plt.tight_layout()\n",
    "\n",
    "output_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/outputs/evaluations/confusion_matrix'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(output_dir, 'conf_mat_gable_AND.png')\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# 7. Close the figure AFTER saving\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nâœ… Confusion matrix plot saved successfully at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49f282-6b79-4627-80bc-4ae49681219d",
   "metadata": {},
   "source": [
    "## OR case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcd4a05-9369-42ad-8a09-e621047dac79",
   "metadata": {},
   "source": [
    "### Single Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18a175d0-40ed-42a3-9d8c-0674b41931ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompt:\n",
      "\n",
      "Given the following building attributes:\n",
      "- Building ID: 3581597.0\n",
      "- Maximum roof slope: 26.21714480174904 degrees\n",
      "- Maximum roof height: 2.562000000000012 meters\n",
      "\n",
      "If the maximum roof slope is greater than or equal to 30 degrees OR the maximum roof height is greater than or equal to 4 meters, predict that the attic is used as a living space.\n",
      "\n",
      "**At the beginning of your response, output only 'Yes' or 'No' on a new line. Do not repeat the input. Do not explain.**\n",
      "\n",
      "Full Model Output:\n",
      "\n",
      "Given the following building attributes:\n",
      "- Building ID: 3581597.0\n",
      "- Maximum roof slope: 26.21714480174904 degrees\n",
      "- Maximum roof height: 2.562000000000012 meters\n",
      "\n",
      "If the maximum roof slope is greater than or equal to 30 degrees OR the maximum roof height is greater than or equal to 4 meters, predict that the attic is used as a living space.\n",
      "\n",
      "**At the beginning of your response, output only 'Yes' or 'No' on a new line. Do not repeat the input. Do not explain.** No\n",
      "Model final prediction: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. Load the CSV\n",
    "df = pd.read_csv('/mnt/data/oe215/env/guna/tabular_image_inference/data/tabular_data/gable_table/attic_ground_truths_gable_3100.csv')\n",
    "\n",
    "# 2. Select the first row\n",
    "row = df.iloc[2]\n",
    "\n",
    "# 3. Create the prompt\n",
    "def create_improved_prompt(row):\n",
    "    return (\n",
    "        f\"Given the following building attributes:\\n\"\n",
    "        f\"- Building ID: {row['gid']}\\n\"\n",
    "        f\"- Maximum roof slope: {row['slope_max']} degrees\\n\"\n",
    "        f\"- Maximum roof height: {row['roof_height_max']} meters\\n\\n\"\n",
    "        \"If the maximum roof slope is greater than or equal to 30 degrees OR the maximum roof height is greater than or equal to 4 meters, \"\n",
    "        \"predict that the attic is used as a living space.\\n\\n\"\n",
    "        \"**At the beginning of your response, output only 'Yes' or 'No' on a new line. Do not repeat the input. Do not explain.**\"\n",
    "    )\n",
    "\n",
    "\n",
    "prompt = create_improved_prompt(row)\n",
    "\n",
    "\n",
    "print(\"Generated Prompt:\\n\")\n",
    "print(prompt)\n",
    "\n",
    "# 4. Perform inference\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "\n",
    "\n",
    "# 5. Decode and just print\n",
    "prediction = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "prediction = prediction.strip()\n",
    "\n",
    "# Print full model output to debug\n",
    "print(\"\\nFull Model Output:\\n\")\n",
    "print(prediction)\n",
    "\n",
    "\n",
    "# After model prediction\n",
    "final_text = prediction.strip().lower()\n",
    "\n",
    "if \" yes\" in final_text or final_text.endswith('yes'):\n",
    "    pred_value = 1\n",
    "elif \" no\" in final_text or final_text.endswith('no'):\n",
    "    pred_value = 0\n",
    "else:\n",
    "    pred_value = -1\n",
    "\n",
    "print(\"Model final prediction:\", pred_value)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ea868-da07-4f19-a9c1-2fc86080884a",
   "metadata": {},
   "source": [
    "### Batch Inference Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7f33dec-d128-4435-9a57-3b552d9d156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference on all rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [01:25<00:00,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All predictions saved successfully to: /mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions/gable_predictions_OR.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "# 1. Load the CSV\n",
    "df = pd.read_csv('/mnt/data/oe215/env/guna/tabular_image_inference/data/tabular_data/gable_table/attic_ground_truths_gable_3100.csv')\n",
    "\n",
    "# 2. Create the improved prompt function\n",
    "def create_improved_prompt(row):\n",
    "    return (\n",
    "        f\"Given the following building attributes:\\n\"\n",
    "        f\"- Building ID: {row['gid']}\\n\"\n",
    "        f\"- Maximum roof slope: {row['slope_max']} degrees\\n\"\n",
    "        f\"- Maximum roof height: {row['roof_height_max']} meters\\n\\n\"\n",
    "        \"If the maximum roof slope is greater than or equal to 30 degrees OR the maximum roof height is greater than or equal to 4 meters, \"\n",
    "        \"predict that the attic is used as a living space.\\n\\n\"\n",
    "        \"**At the beginning of your response, output only 'Yes' or 'No' on a new line. Do not repeat the input. Do not explain.**\"\n",
    "    )\n",
    "\n",
    "# 3. Function to perform prediction for a given prompt\n",
    "def predict_attic_use(prompt):\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "    prediction = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    prediction = prediction.strip()\n",
    "\n",
    "    final_text = prediction.strip().lower()\n",
    "\n",
    "    if \" yes\" in final_text or final_text.endswith('yes'):\n",
    "        return 1\n",
    "    elif \" no\" in final_text or final_text.endswith('no'):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1  # unexpected case\n",
    "\n",
    "# 4. Inference for all rows\n",
    "predictions = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Running inference on all rows\"):\n",
    "    prompt = create_improved_prompt(row)\n",
    "    pred = predict_attic_use(prompt)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# 5. Add predictions to dataframe\n",
    "df['pred_OR'] = predictions\n",
    "\n",
    "# 6. Save the updated dataframe\n",
    "output_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, 'gable_predictions_OR.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… All predictions saved successfully to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00691d1-6802-47e4-b3cf-0b9481849ddf",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33e92f10-b12d-4077-bcde-656310642ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Accuracy: 0.9040\n",
      "\n",
      "ðŸ§® Confusion Matrix (Numbers):\n",
      "[[130   0]\n",
      " [ 36 209]]\n",
      "\n",
      "ðŸ“„ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Used       0.78      1.00      0.88       130\n",
      "        Used       1.00      0.85      0.92       245\n",
      "\n",
      "    accuracy                           0.90       375\n",
      "   macro avg       0.89      0.93      0.90       375\n",
      "weighted avg       0.92      0.90      0.91       375\n",
      "\n",
      "\n",
      "âœ… Confusion matrix plot saved successfully at: /mnt/data/oe215/env/guna/tabular_image_inference/outputs/evaluations/confusion_matrix/conf_mat_gable_OR.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/gjenni-tmpdir-gmWwWw/ipykernel_3476555/3853305158.py:40: UserWarning: Glyph 127968 (\\N{HOUSE BUILDING}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/gjenni-tmpdir-gmWwWw/ipykernel_3476555/3853305158.py:46: UserWarning: Glyph 127968 (\\N{HOUSE BUILDING}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load the predictions CSV\n",
    "df = pd.read_csv('/mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions/gable_predictions_OR.csv')\n",
    "\n",
    "# 2. Define ground truth and model predictions\n",
    "y_true = df['GT_OR']\n",
    "y_pred = df['pred_OR']\n",
    "\n",
    "# 3. Calculate Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nâœ… Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 4. Show Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nðŸ§® Confusion Matrix (Numbers):\")\n",
    "print(cm)\n",
    "\n",
    "# 5. Classification Report\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Not Used\", \"Used\"])\n",
    "print(\"\\nðŸ“„ Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# 4. Create figure\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "\n",
    "# 5. Plot Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Predicted Not Used\", \"Predicted Used\"],\n",
    "            yticklabels=[\"Actual Not Used\", \"Actual Used\"],\n",
    "            ax=ax)\n",
    "ax.set_title('ðŸ  Confusion Matrix for Attic Living Space Prediction')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "\n",
    "# 6. Tight Layout and Save\n",
    "plt.tight_layout()\n",
    "\n",
    "output_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/outputs/evaluations/confusion_matrix'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(output_dir, 'conf_mat_gable_OR.png')\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# 7. Close the figure AFTER saving\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nâœ… Confusion matrix plot saved successfully at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b2d07-cedd-4243-85de-bfd1c3ff49a3",
   "metadata": {},
   "source": [
    "## AND Case - Combined Image and Table Only Based Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561f4ec-880f-4644-913c-32c37443e529",
   "metadata": {},
   "source": [
    "### Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "168b1dc5-baad-4ac6-ac55-a3f2315a6135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference on all rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:29<00:00, 12.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All predictions saved successfully to: /mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions/gable_predictions_table_real_AND.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "# 1. Load the CSV\n",
    "df = pd.read_csv('/mnt/data/oe215/env/guna/tabular_image_inference/data/tabular_data/gable_table/attic_ground_truths_gable_3100_GT_IMG.csv')\n",
    "\n",
    "# 2. Create the improved prompt function\n",
    "def create_improved_prompt(row):\n",
    "    return (\n",
    "        f\"Given the following building attributes:\\n\"\n",
    "        f\"- Building ID: {row['gid']}\\n\"\n",
    "        f\"- Maximum roof slope: {row['slope_max']} degrees\\n\"\n",
    "        f\"- Maximum roof height: {row['roof_height_max']} meters\\n\\n\"\n",
    "        \"If the maximum roof slope is greater than or equal to 30 degrees AND the maximum roof height is greater than or equal to 4 meters, \"\n",
    "        \"predict that the attic is used as a living space.\\n\\n\"\n",
    "        \"**At the beginning of your response, output only 'Yes' or 'No' on a new line. Do not repeat the input. Do not explain.**\"\n",
    "    )\n",
    "\n",
    "# 3. Function to perform prediction for a given prompt\n",
    "def predict_attic_use(prompt):\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "    prediction = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    prediction = prediction.strip()\n",
    "\n",
    "    final_text = prediction.strip().lower()\n",
    "\n",
    "    if \" yes\" in final_text or final_text.endswith('yes'):\n",
    "        return 1\n",
    "    elif \" no\" in final_text or final_text.endswith('no'):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1  # unexpected case\n",
    "\n",
    "# 4. Inference for all rows\n",
    "predictions = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Running inference on all rows\"):\n",
    "    prompt = create_improved_prompt(row)\n",
    "    pred = predict_attic_use(prompt)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# 5. Add predictions to dataframe\n",
    "df['pred_AND'] = predictions\n",
    "\n",
    "# 6. Save the updated dataframe\n",
    "output_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, 'gable_predictions_table_real_AND.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… All predictions saved successfully to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b21c5-1eb3-40fb-8edc-6d3268a37829",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38abd6f0-06bd-4180-a753-70e1b2309829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Accuracy: 0.7360\n",
      "\n",
      "ðŸ§® Confusion Matrix (Numbers):\n",
      "[[183   0]\n",
      " [ 99  93]]\n",
      "\n",
      "ðŸ“„ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Used       0.65      1.00      0.79       183\n",
      "        Used       1.00      0.48      0.65       192\n",
      "\n",
      "    accuracy                           0.74       375\n",
      "   macro avg       0.82      0.74      0.72       375\n",
      "weighted avg       0.83      0.74      0.72       375\n",
      "\n",
      "\n",
      "âœ… Confusion matrix plot saved successfully at: /mnt/data/oe215/env/guna/tabular_image_inference/outputs/evaluations/confusion_matrix/conf_mat_gable_table_real_GT_AND.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load the predictions CSV\n",
    "df = pd.read_csv('/mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions/gable_predictions_table_real_AND.csv')\n",
    "\n",
    "# 2. Define ground truth and model predictions\n",
    "y_true = df['GT_IMG_AND']\n",
    "y_pred = df['pred_AND']\n",
    "\n",
    "# 3. Calculate Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nâœ… Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 4. Show Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nðŸ§® Confusion Matrix (Numbers):\")\n",
    "print(cm)\n",
    "\n",
    "# 5. Classification Report\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Not Used\", \"Used\"])\n",
    "print(\"\\nðŸ“„ Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# 4. Create figure\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "\n",
    "# 5. Plot Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Predicted Not Used\", \"Predicted Used\"],\n",
    "            yticklabels=[\"Actual Not Used\", \"Actual Used\"],\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "\n",
    "# 6. Tight Layout and Save\n",
    "plt.tight_layout()\n",
    "\n",
    "output_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/outputs/evaluations/confusion_matrix'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(output_dir, 'conf_mat_gable_table_real_GT_AND.png')\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# 7. Close the figure AFTER saving\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nâœ… Confusion matrix plot saved successfully at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640586c2-cd9d-4b73-8200-8e15b85212f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9d03a6a-bb76-499c-8c8e-d6cc96649ce2",
   "metadata": {},
   "source": [
    "## OR Case - Combined Image and Table Only Based Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427755ac-a7b1-461e-94c6-b368b109c5d8",
   "metadata": {},
   "source": [
    "### Batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dcf99cb-d98b-49b5-997f-3cba848407de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference on all rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:36<00:00, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All predictions saved successfully to: /mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions/gable_predictions_table_real_OR.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "# 1. Load the CSV\n",
    "df = pd.read_csv('/mnt/data/oe215/env/guna/tabular_image_inference/data/tabular_data/gable_table/attic_ground_truths_gable_3100_GT_IMG.csv')\n",
    "\n",
    "# 2. Create the improved prompt function\n",
    "def create_improved_prompt(row):\n",
    "    return (\n",
    "        f\"Given the following building attributes:\\n\"\n",
    "        f\"- Building ID: {row['gid']}\\n\"\n",
    "        f\"- Maximum roof slope: {row['slope_max']} degrees\\n\"\n",
    "        f\"- Maximum roof height: {row['roof_height_max']} meters\\n\\n\"\n",
    "        \"If the maximum roof slope is greater than or equal to 30 degrees OR the maximum roof height is greater than or equal to 4 meters, \"\n",
    "        \"predict that the attic is used as a living space.\\n\\n\"\n",
    "        \"**At the beginning of your response, output only 'Yes' or 'No' on a new line. Do not repeat the input. Do not explain.**\"\n",
    "    )\n",
    "\n",
    "# 3. Function to perform prediction for a given prompt\n",
    "def predict_attic_use(prompt):\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "    prediction = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    prediction = prediction.strip()\n",
    "\n",
    "    final_text = prediction.strip().lower()\n",
    "\n",
    "    if \" yes\" in final_text or final_text.endswith('yes'):\n",
    "        return 1\n",
    "    elif \" no\" in final_text or final_text.endswith('no'):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1  # unexpected case\n",
    "\n",
    "# 4. Inference for all rows\n",
    "predictions = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Running inference on all rows\"):\n",
    "    prompt = create_improved_prompt(row)\n",
    "    pred = predict_attic_use(prompt)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# 5. Add predictions to dataframe\n",
    "df['pred_OR'] = predictions\n",
    "\n",
    "# 6. Save the updated dataframe\n",
    "output_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, 'gable_predictions_table_real_OR.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… All predictions saved successfully to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d767af0-134a-460f-9472-cb48ed64c08f",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b7d2ca2-a10f-4129-bd17-87e23c8470ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Accuracy: 0.8293\n",
      "\n",
      "ðŸ§® Confusion Matrix (Numbers):\n",
      "[[104   1]\n",
      " [ 63 207]]\n",
      "\n",
      "ðŸ“„ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Used       0.62      0.99      0.76       105\n",
      "        Used       1.00      0.77      0.87       270\n",
      "\n",
      "    accuracy                           0.83       375\n",
      "   macro avg       0.81      0.88      0.82       375\n",
      "weighted avg       0.89      0.83      0.84       375\n",
      "\n",
      "\n",
      "âœ… Confusion matrix plot saved successfully at: /mnt/data/oe215/env/guna/tabular_image_inference/outputs/evaluations/confusion_matrix/conf_mat_gable_table_real_GT_OR.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load the predictions CSV\n",
    "df = pd.read_csv('/mnt/data/oe215/env/guna/tabular_image_inference/outputs/predictions/gable_predictions_table_real_OR.csv')\n",
    "\n",
    "# 2. Define ground truth and model predictions\n",
    "y_true = df['GT_IMG_OR']\n",
    "y_pred = df['pred_OR']\n",
    "\n",
    "# 3. Calculate Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nâœ… Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 4. Show Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nðŸ§® Confusion Matrix (Numbers):\")\n",
    "print(cm)\n",
    "\n",
    "# 5. Classification Report\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Not Used\", \"Used\"])\n",
    "print(\"\\nðŸ“„ Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# 4. Create figure\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "\n",
    "# 5. Plot Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Predicted Not Used\", \"Predicted Used\"],\n",
    "            yticklabels=[\"Actual Not Used\", \"Actual Used\"],\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "\n",
    "# 6. Tight Layout and Save\n",
    "plt.tight_layout()\n",
    "\n",
    "output_dir = '/mnt/data/oe215/env/guna/tabular_image_inference/outputs/evaluations/confusion_matrix'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(output_dir, 'conf_mat_gable_table_real_GT_OR.png')\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# 7. Close the figure AFTER saving\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nâœ… Confusion matrix plot saved successfully at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffa912-6870-4847-91a3-2fcde54e2b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
